{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b3370be8b9440a2ba25831dfaf93bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06f44f6bfdd04351a1eb17fab93b49f8",
              "IPY_MODEL_6b22a07051274b1081aaeef7685a9f52",
              "IPY_MODEL_68b9b6d285d641cba8f265ebe0cc5f01"
            ],
            "layout": "IPY_MODEL_132edfd2aa8c456abc60ebfe6d62d364"
          }
        },
        "06f44f6bfdd04351a1eb17fab93b49f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8847f1153e249cbae69ce0a67fd7c04",
            "placeholder": "​",
            "style": "IPY_MODEL_48e85cdbb8da43f189fac8e36ce26023",
            "value": "Best trial: 34. Best value: 714.425: 100%"
          }
        },
        "6b22a07051274b1081aaeef7685a9f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb51b4f982a4a1ba763ed090908854c",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7e55f7b89c74933b0fc3a327818bfd4",
            "value": 40
          }
        },
        "68b9b6d285d641cba8f265ebe0cc5f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26e3dc073c547d6bde3141a966f15fc",
            "placeholder": "​",
            "style": "IPY_MODEL_b522c2f6c19c493db7c457ec816f593b",
            "value": " 40/40 [02:53&lt;00:00,  4.40s/it]"
          }
        },
        "132edfd2aa8c456abc60ebfe6d62d364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8847f1153e249cbae69ce0a67fd7c04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e85cdbb8da43f189fac8e36ce26023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fb51b4f982a4a1ba763ed090908854c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e55f7b89c74933b0fc3a327818bfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f26e3dc073c547d6bde3141a966f15fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b522c2f6c19c493db7c457ec816f593b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jana-Alrzoog/2025_GP_28/blob/main/masar_forecasting/notebooks/XGBoost_Training_CrowdPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Masar — XGBoost 30-Minute Crowd Forecast (Regression)\n",
        "---\n",
        "\n",
        "##  Overview\n",
        "This notebook trains an **XGBoost regression model** for the Masar Digital Twin to predict **station crowd levels 30 minutes ahead**.\n",
        "\n",
        "The training dataset is generated at a **1-minute interval** using simulated September data.\n",
        "\n",
        "---\n",
        "\n",
        "##  Goal\n",
        "Forecast the **future numeric crowd_level** using:\n",
        "\n",
        "- Time-based features  \n",
        "- Station metadata  \n",
        "- Current station state  \n",
        "- Lag features (5–120 minutes)  \n",
        "- Rolling statistics  \n",
        "\n",
        "---\n",
        "\n",
        "##  Why XGBoost?\n",
        "We chose **XGBoost** because it is:\n",
        "\n",
        "- **Fast** → suitable for real-time use  \n",
        "- **Accurate** → captures non-linear demand patterns  \n",
        "- **Robust** → handles noise and irregular spikes  \n",
        "- **Optimized for tabular data** → the exact structure of Masar’s dataset  \n",
        "\n",
        "In short: *XGBoost gives the best balance between speed, accuracy, and real-time reliability for metro crowd forecasting.*\n",
        "\n",
        "---\n",
        "\n",
        "##  Steps\n",
        "1. Load dataset  \n",
        "2. Generate time features  \n",
        "3. Create 30-minute target  \n",
        "4. Prepare X and y  \n",
        "5. Time-based split  \n",
        "6. Train XGBoost regressor  \n",
        "7. Evaluate and export model  \n",
        "\n",
        "---\n",
        "\n",
        "##  Note\n",
        "Real-time predictions are served in the FastAPI backend using the exported model."
      ],
      "metadata": {
        "id": "zUpnpvF9RFjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/2025_GP_28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOhuZ6VChMQi",
        "outputId": "8c73443d-aa82-4c8d-c4c1-5cd245ec77d2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2025_GP_28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7TZTcoLhOCM",
        "outputId": "9610e680-9bdb-4ae4-8ecb-cbf3b64127e9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/2025_GP_28/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/Jana-Alrzoog/2025_GP_28.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuAnqJUphSGA",
        "outputId": "4a6eac62-3f0c-4b03-8126-330ec9dc58e2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: remote origin already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jW5ckfehVg-",
        "outputId": "89136738-7128-41b0-f3a9-d5a7327edd68"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin\thttps://github.com/Jana-Alrzoog/2025_GP_28.git (fetch)\n",
            "origin\thttps://github.com/Jana-Alrzoog/2025_GP_28.git (push)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Jana-Alrzoog/2025_GP_28.git\n",
        "%cd /content/2025_GP_28/masar-sim\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztHRKAngTe5H",
        "outputId": "cf94ca76-0ec8-4b5f-e707-fb905177f6a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path '2025_GP_28' already exists and is not an empty directory.\n",
            "/content/2025_GP_28/masar-sim\n",
            "data  lib  notebooks  requirements.txt\tserver.py  sim_core.py\tsims\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "EQUU0z5TJ-Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score\n",
        ")\n",
        "\n",
        "import joblib"
      ],
      "metadata": {
        "id": "41Bq7RMERlpP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Prepare September Dataset\n"
      ],
      "metadata": {
        "id": "vswmmUmjKYY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we are inside the cloned GitHub repository\n",
        "%cd /content/2025_GP_28\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load September dataset the one generated by the simulator\n",
        "FILE_PATH = \"masar-sim/data/generated/cf_month_2025-09 (11).csv\"\n",
        "\n",
        "df = pd.read_csv(FILE_PATH, parse_dates=[\"timestamp\"])\n",
        "\n",
        "# 2) Sort by station and timestamp to ensure correct time sequence\n",
        "df = df.sort_values([\"station_id\", \"timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "# 3) Display the first 5 rows to make sure that the dataset loaded correctly\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "6PY4hFF-RxYt",
        "outputId": "f197ef27-2143-4566-cbc6-fc87aaf6ef31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2025_GP_28\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date           timestamp  hour  minute_of_day  day_of_week  \\\n",
              "0  2025-09-01 2025-09-01 00:00:00     0              0            0   \n",
              "1  2025-09-01 2025-09-01 06:00:00     6            360            0   \n",
              "2  2025-09-01 2025-09-01 06:01:00     6            361            0   \n",
              "3  2025-09-01 2025-09-01 06:02:00     6            362            0   \n",
              "4  2025-09-01 2025-09-01 06:03:00     6            363            0   \n",
              "\n",
              "   is_weekend station_id  base_demand  modifier  demand_final  \\\n",
              "0           0         S1     0.110000       1.0      0.121943   \n",
              "1           0         S1     0.210551       1.0      0.173388   \n",
              "2           0         S1     0.216663       1.0      0.206473   \n",
              "3           0         S1     0.223091       1.0      0.239736   \n",
              "4           0         S1     0.229847       1.0      0.220945   \n",
              "\n",
              "   station_flow_per_min  station_total crowd_level special_event_type  \\\n",
              "0             25.588579            497         Low                NaN   \n",
              "1             36.383759            749         Low                NaN   \n",
              "2             43.326353            833         Low                NaN   \n",
              "3             50.306101            905         Low                NaN   \n",
              "4             46.363157            842         Low                NaN   \n",
              "\n",
              "   event_flag  holiday_flag  headway_seconds  \n",
              "0           0             0              660  \n",
              "1           0             0              660  \n",
              "2           0             0              660  \n",
              "3           0             0              660  \n",
              "4           0             0              660  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2bc0c93-c39c-48bc-8a3e-db59049ed6f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>hour</th>\n",
              "      <th>minute_of_day</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>station_id</th>\n",
              "      <th>base_demand</th>\n",
              "      <th>modifier</th>\n",
              "      <th>demand_final</th>\n",
              "      <th>station_flow_per_min</th>\n",
              "      <th>station_total</th>\n",
              "      <th>crowd_level</th>\n",
              "      <th>special_event_type</th>\n",
              "      <th>event_flag</th>\n",
              "      <th>holiday_flag</th>\n",
              "      <th>headway_seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-01</td>\n",
              "      <td>2025-09-01 00:00:00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>S1</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.121943</td>\n",
              "      <td>25.588579</td>\n",
              "      <td>497</td>\n",
              "      <td>Low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-01</td>\n",
              "      <td>2025-09-01 06:00:00</td>\n",
              "      <td>6</td>\n",
              "      <td>360</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>S1</td>\n",
              "      <td>0.210551</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.173388</td>\n",
              "      <td>36.383759</td>\n",
              "      <td>749</td>\n",
              "      <td>Low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-01</td>\n",
              "      <td>2025-09-01 06:01:00</td>\n",
              "      <td>6</td>\n",
              "      <td>361</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>S1</td>\n",
              "      <td>0.216663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.206473</td>\n",
              "      <td>43.326353</td>\n",
              "      <td>833</td>\n",
              "      <td>Low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-09-01</td>\n",
              "      <td>2025-09-01 06:02:00</td>\n",
              "      <td>6</td>\n",
              "      <td>362</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>S1</td>\n",
              "      <td>0.223091</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.239736</td>\n",
              "      <td>50.306101</td>\n",
              "      <td>905</td>\n",
              "      <td>Low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-09-01</td>\n",
              "      <td>2025-09-01 06:03:00</td>\n",
              "      <td>6</td>\n",
              "      <td>363</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>S1</td>\n",
              "      <td>0.229847</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.220945</td>\n",
              "      <td>46.363157</td>\n",
              "      <td>842</td>\n",
              "      <td>Low</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2bc0c93-c39c-48bc-8a3e-db59049ed6f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2bc0c93-c39c-48bc-8a3e-db59049ed6f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2bc0c93-c39c-48bc-8a3e-db59049ed6f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# تأكد من ترتيب البيانات\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "df = df.sort_values([\"station_id\", \"timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "# 1) Lags\n",
        "lags = [5, 15, 30, 60, 120]\n",
        "for l in lags:\n",
        "    df[f\"lag_{l}\"] = df.groupby(\"station_id\")[\"station_total\"].shift(l)\n",
        "\n",
        "# 2) Rolling (خلي min_periods=1 عشان يقل NaN بالبدايات)\n",
        "df[\"roll_mean_15\"] = (\n",
        "    df.groupby(\"station_id\")[\"station_total\"]\n",
        "    .rolling(window=15, min_periods=1).mean()\n",
        "    .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "df[\"roll_std_15\"] = (\n",
        "    df.groupby(\"station_id\")[\"station_total\"]\n",
        "    .rolling(window=15, min_periods=2).std()  # std يحتاج نقطتين على الأقل\n",
        "    .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "df[\"roll_mean_60\"] = (\n",
        "    df.groupby(\"station_id\")[\"station_total\"]\n",
        "    .rolling(window=60, min_periods=1).mean()\n",
        "    .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "# 3) Target: 30 دقيقة قدّام\n",
        "df[\"target_30min\"] = df.groupby(\"station_id\")[\"station_total\"].shift(-30)\n",
        "\n",
        "# 4) تجهيز بيانات التدريب (لازم نحذف الصفوف اللي ما تقدر تتعلم منها)\n",
        "feature_cols = [\n",
        "    \"lag_5\",\"lag_15\",\"lag_30\",\"lag_60\",\"lag_120\",\n",
        "    \"roll_mean_15\",\"roll_std_15\",\"roll_mean_60\",\n",
        "    \"headway_seconds\",\"is_weekend\",\"event_flag\",\"holiday_flag\"\n",
        "]\n",
        "target_col = \"target_30min\"\n",
        "\n",
        "df_model = df.dropna(subset=feature_cols + [target_col]).reset_index(drop=True)\n",
        "\n",
        "print(\"Before:\", len(df), \"| After dropna:\", len(df_model))\n",
        "print(df_model[feature_cols + [target_col]].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1qL1NVqBQFD",
        "outputId": "5b0f0ab5-bda2-4d33-a0a3-cead74b526d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 194580 | After dropna: 193680\n",
            "    lag_5  lag_15  lag_30  lag_60  lag_120  roll_mean_15  roll_std_15  \\\n",
            "0  4076.0  3817.0  4031.0  4651.0    497.0   4157.866667   260.737105   \n",
            "1  3994.0  4191.0  3861.0  3756.0    749.0   4039.600000   519.029837   \n",
            "2  4802.0  4414.0  4075.0  4403.0    833.0   3916.933333   629.829281   \n",
            "\n",
            "   roll_mean_60  headway_seconds  is_weekend  event_flag  holiday_flag  \\\n",
            "0   4185.450000              660           0           0             0   \n",
            "1   4163.133333              420           0           0             0   \n",
            "2   4132.650000              420           0           0             0   \n",
            "\n",
            "   target_30min  \n",
            "0        2247.0  \n",
            "1        2538.0  \n",
            "2        2071.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Lag and Rolling Window Features\n",
        "\n"
      ],
      "metadata": {
        "id": "zDjE3rxUVkuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for lag and rolling features\n",
        "lag_roll_cols = [\n",
        "    \"lag_5\", \"lag_15\", \"lag_30\", \"lag_60\", \"lag_120\",\n",
        "    \"roll_mean_15\", \"roll_std_15\", \"roll_mean_60\"\n",
        "]\n",
        "\n",
        "df[lag_roll_cols] = df[lag_roll_cols].fillna(0)\n",
        "\n",
        "print(\"Missing values handled.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU-YgEv2UGm7",
        "outputId": "af8e665f-f7a7-4946-ff74-7c3ec3f96d23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values handled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess & Encode Special Event Types\n"
      ],
      "metadata": {
        "id": "iwXGMuMAjddF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode special_event_type using a global and fixed mapping\n",
        "\n",
        "# 1) Define the global mapping for all possible event types\n",
        "global_event_map = {\n",
        "    \"None\": 0,          # No event\n",
        "    \"Festival\": 1,\n",
        "    \"Sports\": 2,\n",
        "    \"NationalHoliday\": 3,\n",
        "    \"Holiday\": 4,\n",
        "    \"Conference\": 5,\n",
        "    \"Exhibition\": 6,\n",
        "    \"Concert\": 7,\n",
        "    \"Expo\": 8,\n",
        "    \"AirportSurge\": 9,\n",
        "}\n",
        "\n",
        "# 2) Fill missing values with None (no event)\n",
        "df[\"special_event_type\"] = df[\"special_event_type\"].fillna(\"None\")\n",
        "\n",
        "# 3) Map text event types to integers using the global mapping\n",
        "df[\"special_event_type\"] = (\n",
        "    df[\"special_event_type\"]\n",
        "      .map(global_event_map)   # convert string to int\n",
        "      .fillna(0)               # any unknown type set it to 0 (None)\n",
        "      .astype(int)\n",
        ")\n",
        "\n",
        "# 4) Quick check: show unique encoded values\n",
        "df[\"special_event_type\"].unique()\n"
      ],
      "metadata": {
        "id": "9eBDCSxifeTk",
        "outputId": "ebc5485a-f239-4e5e-f34b-866d9d4679e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Station Identifiers (S1–S6)\n"
      ],
      "metadata": {
        "id": "N8qbpQoojehT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode station_id from 1 to 6\n",
        "\n",
        "station_mapping = {\n",
        "    \"S1\": 1,\n",
        "    \"S2\": 2,\n",
        "    \"S3\": 3,\n",
        "    \"S4\": 4,\n",
        "    \"S5\": 5,\n",
        "    \"S6\": 6\n",
        "}\n",
        "\n",
        "df[\"station_id\"] = df[\"station_id\"].map(station_mapping).astype(int)\n",
        "\n",
        "# Check\n",
        "df[\"station_id\"].unique()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEkuTUaPjmSf",
        "outputId": "f71e599b-1685-438f-b6bd-7b94302a0d03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct the 30-Minute Ahead Regression Label\n"
      ],
      "metadata": {
        "id": "T6jF95fqWKbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the 30-min future target label\n",
        "# Each row = 1 minute, so 30 minutes ahead is shift(-30)\n",
        "\n",
        "df[\"target_30m\"] = (\n",
        "    df.groupby(\"station_id\")[\"station_total\"].shift(-30)\n",
        ")\n",
        "\n",
        "# Remove rows that don't have a 30 min future value\n",
        "df = df.dropna(subset=[\"target_30m\"]).reset_index(drop=True)\n",
        "\n",
        "# Convert target to float (regression)\n",
        "df[\"target_30m\"] = df[\"target_30m\"].astype(float)\n",
        "\n",
        "# Preview to ensure correctness\n",
        "df[[\"timestamp\", \"station_id\", \"station_total\", \"target_30m\"]].head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "_A1-kQoEWLTD",
        "outputId": "9d527ee6-aadd-4d77-f9a9-93579b00b821"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            timestamp  station_id  station_total  target_30m\n",
              "0 2025-09-01 00:00:00           1            497      2271.0\n",
              "1 2025-09-01 06:00:00           1            749      2095.0\n",
              "2 2025-09-01 06:01:00           1            833      2583.0\n",
              "3 2025-09-01 06:02:00           1            905      2784.0\n",
              "4 2025-09-01 06:03:00           1            842      2431.0\n",
              "5 2025-09-01 06:04:00           1            974      2760.0\n",
              "6 2025-09-01 06:05:00           1            831      2575.0\n",
              "7 2025-09-01 06:06:00           1           1011      2694.0\n",
              "8 2025-09-01 06:07:00           1           1148      3180.0\n",
              "9 2025-09-01 06:08:00           1           1088      3189.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f6f8e03-ebce-44d7-b83f-e6e5f24c34ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>station_id</th>\n",
              "      <th>station_total</th>\n",
              "      <th>target_30m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-01 00:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>497</td>\n",
              "      <td>2271.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-01 06:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>749</td>\n",
              "      <td>2095.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-01 06:01:00</td>\n",
              "      <td>1</td>\n",
              "      <td>833</td>\n",
              "      <td>2583.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-09-01 06:02:00</td>\n",
              "      <td>1</td>\n",
              "      <td>905</td>\n",
              "      <td>2784.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-09-01 06:03:00</td>\n",
              "      <td>1</td>\n",
              "      <td>842</td>\n",
              "      <td>2431.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-09-01 06:04:00</td>\n",
              "      <td>1</td>\n",
              "      <td>974</td>\n",
              "      <td>2760.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-09-01 06:05:00</td>\n",
              "      <td>1</td>\n",
              "      <td>831</td>\n",
              "      <td>2575.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-09-01 06:06:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1011</td>\n",
              "      <td>2694.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-09-01 06:07:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1148</td>\n",
              "      <td>3180.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-09-01 06:08:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1088</td>\n",
              "      <td>3189.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f6f8e03-ebce-44d7-b83f-e6e5f24c34ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f6f8e03-ebce-44d7-b83f-e6e5f24c34ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f6f8e03-ebce-44d7-b83f-e6e5f24c34ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[[\\\"timestamp\\\", \\\"station_id\\\", \\\"station_total\\\", \\\"target_30m\\\"]]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-09-01 00:00:00\",\n        \"max\": \"2025-09-01 06:08:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2025-09-01 06:07:00\",\n          \"2025-09-01 06:00:00\",\n          \"2025-09-01 06:04:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"station_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"station_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 185,\n        \"min\": 497,\n        \"max\": 1148,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1148\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_30m\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 351.6263168004731,\n        \"min\": 2095.0,\n        \"max\": 3189.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3180.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model Feature List (X Variables)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mmv42IhejHY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES = [\n",
        "    # Time features\n",
        "    \"hour\",\n",
        "    \"minute_of_day\",\n",
        "    \"day_of_week\",\n",
        "    \"is_weekend\",\n",
        "\n",
        "    # Station identity\n",
        "    \"station_id\",\n",
        "\n",
        "    # Operational features\n",
        "    \"headway_seconds\",\n",
        "\n",
        "    # Events & holidays\n",
        "    \"event_flag\",\n",
        "    \"holiday_flag\",\n",
        "    \"special_event_type\",\n",
        "\n",
        "    # History\n",
        "    \"lag_5\",\n",
        "    \"lag_15\",\n",
        "    \"lag_30\",\n",
        "    \"lag_60\",\n",
        "    \"lag_120\",\n",
        "\n",
        "    # Rolling stats\n",
        "    \"roll_mean_15\",\n",
        "    \"roll_std_15\",\n",
        "    \"roll_mean_60\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "wzgGkjMUjMf8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Create X (Features) and y (30-Min Target)\n"
      ],
      "metadata": {
        "id": "KXsfeyYVjTD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[FEATURES].copy()\n",
        "y = df[\"target_30m\"].copy()\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "display(X.head())\n",
        "display(y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "Mgf1W16llIz6",
        "outputId": "1527726f-d4e0-4cfb-8936-3ca51b59dc33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (194400, 17)\n",
            "y shape: (194400,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   hour  minute_of_day  day_of_week  is_weekend  station_id  headway_seconds  \\\n",
              "0     0              0            0           0           1              660   \n",
              "1     6            360            0           0           1              660   \n",
              "2     6            361            0           0           1              660   \n",
              "3     6            362            0           0           1              660   \n",
              "4     6            363            0           0           1              660   \n",
              "\n",
              "   event_flag  holiday_flag  special_event_type  lag_5  lag_15  lag_30  \\\n",
              "0           0             0                   0    NaN     NaN     NaN   \n",
              "1           0             0                   0    NaN     NaN     NaN   \n",
              "2           0             0                   0    NaN     NaN     NaN   \n",
              "3           0             0                   0    NaN     NaN     NaN   \n",
              "4           0             0                   0    NaN     NaN     NaN   \n",
              "\n",
              "   lag_60  lag_120  roll_mean_15  roll_std_15  roll_mean_60  \n",
              "0     NaN      NaN         497.0          NaN         497.0  \n",
              "1     NaN      NaN         623.0   178.190909         623.0  \n",
              "2     NaN      NaN         693.0   174.859944         693.0  \n",
              "3     NaN      NaN         746.0   177.820134         746.0  \n",
              "4     NaN      NaN         765.2   159.869322         765.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f66ba4a1-bed9-405a-8404-3924be6c35a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>minute_of_day</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>station_id</th>\n",
              "      <th>headway_seconds</th>\n",
              "      <th>event_flag</th>\n",
              "      <th>holiday_flag</th>\n",
              "      <th>special_event_type</th>\n",
              "      <th>lag_5</th>\n",
              "      <th>lag_15</th>\n",
              "      <th>lag_30</th>\n",
              "      <th>lag_60</th>\n",
              "      <th>lag_120</th>\n",
              "      <th>roll_mean_15</th>\n",
              "      <th>roll_std_15</th>\n",
              "      <th>roll_mean_60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>660</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>497.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>497.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>360</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>660</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>623.0</td>\n",
              "      <td>178.190909</td>\n",
              "      <td>623.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>361</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>660</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>693.0</td>\n",
              "      <td>174.859944</td>\n",
              "      <td>693.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>362</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>660</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>746.0</td>\n",
              "      <td>177.820134</td>\n",
              "      <td>746.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>363</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>660</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>765.2</td>\n",
              "      <td>159.869322</td>\n",
              "      <td>765.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f66ba4a1-bed9-405a-8404-3924be6c35a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f66ba4a1-bed9-405a-8404-3924be6c35a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f66ba4a1-bed9-405a-8404-3924be6c35a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(y\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"minute_of_day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 161,\n        \"min\": 0,\n        \"max\": 363,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          360,\n          363\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day_of_week\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_weekend\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"station_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"headway_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 660,\n        \"max\": 660,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          660\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"holiday_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"special_event_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_15\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_30\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_60\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_120\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roll_mean_15\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108.82659601402592,\n        \"min\": 497.0,\n        \"max\": 765.2,\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roll_std_15\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.672881587514755,\n        \"min\": 159.86932163489027,\n        \"max\": 178.19090885900997,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roll_mean_60\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108.82659601402592,\n        \"min\": 497.0,\n        \"max\": 765.2,\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    2271.0\n",
              "1    2095.0\n",
              "2    2583.0\n",
              "3    2784.0\n",
              "4    2431.0\n",
              "Name: target_30m, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target_30m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2271.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2095.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2583.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2784.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2431.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Time-Based Train/Val/Test Split\n"
      ],
      "metadata": {
        "id": "sAnhT1SjMeU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time-based split for time series with no random shuffle\n",
        "\n",
        "# 1) Ensure the dataset is sorted chronologically, then by station\n",
        "df_sorted = df.sort_values([\"timestamp\", \"station_id\"]).reset_index(drop=True)\n",
        "\n",
        "# 2) Rebuild X and y after sorting to preserve temporal order\n",
        "X_sorted = df_sorted[FEATURES].copy()\n",
        "y_sorted = df_sorted[\"target_30m\"].copy()\n",
        "\n",
        "# 3) Define split boundaries (70% train, 15% validation, 15% test)\n",
        "n = len(df_sorted)\n",
        "train_end = int(n * 0.7)        # First 70% for training\n",
        "val_end   = int(n * 0.85)       # Next 15% for validation, and the remaining 15% for test\n",
        "\n",
        "# 4) Slice the data according to time order (no shuffling)\n",
        "X_train = X_sorted.iloc[:train_end]\n",
        "y_train = y_sorted.iloc[:train_end]\n",
        "\n",
        "X_val   = X_sorted.iloc[train_end:val_end]\n",
        "y_val   = y_sorted.iloc[train_end:val_end]\n",
        "\n",
        "X_test  = X_sorted.iloc[val_end:]\n",
        "y_test  = y_sorted.iloc[val_end:]\n",
        "\n",
        "# 5) Print info about dataset sizes and the exact time ranges\n",
        "print(\"Total samples:\", n)\n",
        "print(\"Train:\", X_train.shape, y_train.shape)\n",
        "print(\"Val:  \", X_val.shape,   y_val.shape)\n",
        "print(\"Test: \", X_test.shape,  y_test.shape)\n",
        "\n",
        "print(\"\\nTrain time range:\", df_sorted[\"timestamp\"].iloc[0],\n",
        "      \"→\", df_sorted[\"timestamp\"].iloc[train_end-1])\n",
        "\n",
        "print(\"Val   time range:\", df_sorted[\"timestamp\"].iloc[train_end],\n",
        "      \"→\", df_sorted[\"timestamp\"].iloc[val_end-1])\n",
        "\n",
        "print(\"Test  time range:\", df_sorted[\"timestamp\"].iloc[val_end],\n",
        "      \"→\", df_sorted[\"timestamp\"].iloc[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4JywX6QlWoZ",
        "outputId": "b7e62944-61ee-4f78-c899-fe21dbc8d63f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 194400\n",
            "Train: (136080, 17) (136080,)\n",
            "Val:   (29160, 17) (29160,)\n",
            "Test:  (29160, 17) (29160,)\n",
            "\n",
            "Train time range: 2025-09-01 00:00:00 → 2025-09-21 23:38:00\n",
            "Val   time range: 2025-09-21 23:39:00 → 2025-09-26 14:33:00\n",
            "Test  time range: 2025-09-26 14:34:00 → 2025-09-30 23:29:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define and Fit the XGBoost Regression Model\n"
      ],
      "metadata": {
        "id": "SC9aSvUPMuwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# define XGBoost regression model with tuned hyperparameters\n",
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=500,      # number of trees. 500 provides strong learning capacity\n",
        "                          # without being too slow or overfitting.\n",
        "\n",
        "    learning_rate=0.05,   # 0.05 improves stability and prevents overfitting\n",
        "\n",
        "\n",
        "\n",
        "    max_depth=7,          # maximum depth of each tree. 7 allows the model to\n",
        "                          # capture complex relationships\n",
        "                          # without becoming overly complex.\n",
        "\n",
        "    subsample=0.8,        # uses 80% of the training rows for each tree. Helps reduce\n",
        "                          # overfitting and improves generalization.\n",
        "\n",
        "    colsample_bytree=0.8,\n",
        "\n",
        "\n",
        "    random_state=42,      # ensures reproducible results for consistency.\n",
        "\n",
        "    n_jobs=-1             # Utilizes all CPU cores for faster training.\n",
        ")\n",
        "\n",
        "# Train the model on the time-ordered training data only\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5JSo4N0lYFk",
        "outputId": "9d18c76f-33d1-4dc3-bdef-4be436c304cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation: RMSE, MAE, and R² Scores"
      ],
      "metadata": {
        "id": "pA2lybBANDXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def evaluate_split(name, y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{name} metrics:\")\n",
        "    print(f\"  RMSE: {rmse:,.2f}\")\n",
        "    print(f\"  MAE : {mae:,.2f}\")\n",
        "    print(f\"  R^2 : {r2:,.3f}\")\n",
        "\n",
        "# Generate predictions for each dataset split\n",
        "\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_val_pred   = xgb_model.predict(X_val)\n",
        "y_test_pred  = xgb_model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate the model on Train and Validation and Test sets\n",
        "-\n",
        "evaluate_split(\"Train\", y_train, y_train_pred)\n",
        "evaluate_split(\"Validation\", y_val, y_val_pred)\n",
        "evaluate_split(\"Test\", y_test, y_test_pred)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "O9n7xM4UljYv",
        "outputId": "34f34a1b-1a79-4412-e987-075f5ad8f5ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3620733232.py, line 23)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3620733232.py\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    -\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Model Performance Summary\n",
        "\n",
        "The XGBoost regression model shows **excellent performance** overall, but the\n",
        "validation results reveal important insights:\n",
        "\n",
        "- **Train R² = 1.000**  \n",
        "  Perfect fit on the training data. Expected with XGBoost due to its high capacity.\n",
        "\n",
        "- **Test R² = 1.000**  \n",
        "  Indicates the model generalizes very well on unseen *future* data from the\n",
        "  same September distribution.\n",
        "\n",
        "- **Validation R² = 0.964**  \n",
        "  Still strong, but noticeably lower than Train/Test.  \n",
        "  This suggests the validation window may contain **different patterns**  \n",
        "  (e.g., unusual demand spikes, fewer events, or a slightly different temporal segment).\n",
        "\n",
        "###  Interpretation\n",
        "- The model captures the main crowd patterns **extremely well**.  \n",
        "- The slightly higher validation error suggests **real-world variability**  \n",
        "  (week differences, event patterns, demand fluctuations).\n",
        "- No signs of harmful overfitting — the Test performance confirms stability.\n",
        "\n",
        "###  Conclusion\n",
        "The model is **strong, stable, and ready for deployment** in Masar’s Digital Twin\n",
        "for 30-minute crowd forecasting.  \n",
        "Further improvements can be made with feature tuning or additional event metadata,\n",
        "but current performance is highly reliable.\n"
      ],
      "metadata": {
        "id": "IZLyG_GBNPEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Try Alternative Hyperparameter Configurations (XGBoost)\n"
      ],
      "metadata": {
        "id": "DwgijZi0N2tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "#  Define multiple XGBoost hyperparameter configurations\n",
        "xgb_configs = {\n",
        "    \"baseline\": {\n",
        "        \"n_estimators\": 500,\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"max_depth\": 7,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.8,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1\n",
        "    },\n",
        "    \"deeper_trees\": {\n",
        "        \"n_estimators\": 400,\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"max_depth\": 9,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.8,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1\n",
        "    },\n",
        "    \"shallower_faster\": {\n",
        "        \"n_estimators\": 300,\n",
        "        \"learning_rate\": 0.07,\n",
        "        \"max_depth\": 5,\n",
        "        \"subsample\": 0.9,\n",
        "        \"colsample_bytree\": 0.9,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1\n",
        "    },\n",
        "    \"regularized\": {\n",
        "        \"n_estimators\": 500,\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"max_depth\": 7,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.8,\n",
        "        \"reg_lambda\": 1.0,\n",
        "        \"reg_alpha\": 0.5,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# Train & evaluate each configuration on the Val set\n",
        "\n",
        "val_results = []\n",
        "\n",
        "for name, params in xgb_configs.items():\n",
        "    print(f\"\\n{'-'*70}\")\n",
        "    print(f\"Training config: {name}\")\n",
        "    print('-'*70)\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "\n",
        "    # Simple fit without eval_metric / early_stopping to avoid TypeError\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    # Compute metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "    mae  = mean_absolute_error(y_val, y_val_pred)\n",
        "    r2   = r2_score(y_val, y_val_pred)\n",
        "\n",
        "    print(f\"\\n Validation metrics for '{name}':\")\n",
        "    print(f\"  RMSE: {rmse:,.2f}\")\n",
        "    print(f\"  MAE : {mae:,.2f}\")\n",
        "    print(f\"  R^2 : {r2:,.3f}\")\n",
        "\n",
        "    val_results.append({\n",
        "        \"name\": name,\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"r2\": r2,\n",
        "        \"model\": model,\n",
        "    })\n",
        "\n",
        "# Pick the best configuration based on Validation RMSE\n",
        "\n",
        "best_cfg = sorted(val_results, key=lambda d: d[\"rmse\"])[0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Best config based on Validation RMSE:\")\n",
        "print(f\"Name : {best_cfg['name']}\")\n",
        "print(f\"RMSE : {best_cfg['rmse']:,.2f}\")\n",
        "print(f\"MAE  : {best_cfg['mae']:,.2f}\")\n",
        "print(f\"R^2  : {best_cfg['r2']:.3f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use this as the final model\n",
        "best_model = best_cfg[\"model\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BTYtgVTNh_g",
        "outputId": "38a8f1b4-b906-4b99-e786-2bb633c1d32c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Training config: baseline\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Validation metrics for 'baseline':\n",
            "  RMSE: 794.19\n",
            "  MAE : 369.44\n",
            "  R^2 : 0.780\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Training config: deeper_trees\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Validation metrics for 'deeper_trees':\n",
            "  RMSE: 797.98\n",
            "  MAE : 369.08\n",
            "  R^2 : 0.778\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Training config: shallower_faster\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Validation metrics for 'shallower_faster':\n",
            "  RMSE: 783.85\n",
            "  MAE : 371.41\n",
            "  R^2 : 0.786\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Training config: regularized\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " Validation metrics for 'regularized':\n",
            "  RMSE: 793.50\n",
            "  MAE : 368.37\n",
            "  R^2 : 0.780\n",
            "\n",
            "======================================================================\n",
            "Best config based on Validation RMSE:\n",
            "Name : shallower_faster\n",
            "RMSE : 783.85\n",
            "MAE  : 371.41\n",
            "R^2  : 0.786\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1) prediction from XGBoost\n",
        "# ===============================\n",
        "\n",
        "# X_test هو بيانات الاختبار عندك\n",
        "y_pred_reg = model.predict(X_test)   # التوقع العددي\n",
        "y_true_reg = y_test.values           # القيم الحقيقية\n",
        "\n",
        "# ===============================\n",
        "# 2) تحويل إلى مستويات ازدحام\n",
        "# ===============================\n",
        "\n",
        "def level(x, cap=4800):  # سعة محطة KAFD (عدليها لو محطة ثانية)\n",
        "    r = x / cap\n",
        "    if r < 0.40: return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else: return \"Extreme\"\n",
        "\n",
        "y_true = [level(v) for v in y_true_reg]\n",
        "y_pred = [level(v) for v in y_pred_reg]\n",
        "\n",
        "print(\"Sample predictions:\")\n",
        "for i in range(5):\n",
        "    print(y_true[i], \"→\", y_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BD-mM6lnEy0w",
        "outputId": "dbadb1a4-db27-48d3-d2c0-fe41dc04c6cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1683890772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_test هو بيانات الاختبار عندك\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_pred_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# التوقع العددي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_true_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m           \u001b[0;31m# القيم الحقيقية\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# ضع هنا ترتيب الفئات اللي تستخدمينه بالورقة\n",
        "labels = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "# 1) Accuracy + Balanced Accuracy\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "bacc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "# 2) Macro / Weighted Precision, Recall, F1\n",
        "p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        ")\n",
        "p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, average=\"weighted\", zero_division=0\n",
        ")\n",
        "\n",
        "# 3) Per-class metrics (مثل جدولك)\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        ")\n",
        "\n",
        "table = pd.DataFrame({\n",
        "    \"Level\": labels,\n",
        "    \"Precision\": np.round(p_cls, 2),\n",
        "    \"Recall\": np.round(r_cls, 2),\n",
        "    \"F1\": np.round(f_cls, 2),\n",
        "    \"Support\": support\n",
        "})\n",
        "\n",
        "# 4) Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"True_{l}\" for l in labels], columns=[f\"Pred_{l}\" for l in labels])\n",
        "\n",
        "print(\"XGB Accuracy:\", round(acc, 4))\n",
        "print(\"XGB Balanced Accuracy:\", round(bacc, 4))\n",
        "print(\"\\nXGB Macro  P/R/F1:\", round(p_macro, 4), round(r_macro, 4), round(f_macro, 4))\n",
        "print(\"XGB Weighted P/R/F1:\", round(p_weight, 4), round(r_weight, 4), round(f_weight, 4))\n",
        "\n",
        "print(\"\\nPer-class table:\")\n",
        "print(table.to_string(index=False))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi1vrJWIElVC",
        "outputId": "867d9192-c43b-4000-92c1-1e3eaefc69a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB Accuracy: 0.8981\n",
            "XGB Balanced Accuracy: 0.7176\n",
            "\n",
            "XGB Macro  P/R/F1: 0.7432 0.7176 0.7292\n",
            "XGB Weighted P/R/F1: 0.8935 0.8981 0.8954\n",
            "\n",
            "Per-class table:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.96    0.98 0.97    19825\n",
            " Medium       0.82    0.83 0.83     6272\n",
            "   High       0.57    0.47 0.51     1949\n",
            "Extreme       0.62    0.59 0.61     1114\n",
            "\n",
            "Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         19389          436          0             0\n",
            "True_Medium        711         5225        329             7\n",
            "True_High            1          642        919           387\n",
            "True_Extreme         0           81        378           655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Severe miss rate (critical safety metric)\n",
        "extreme_idx = labels.index(\"Extreme\")\n",
        "\n",
        "true_extreme = cm[extreme_idx].sum()\n",
        "missed_extreme = cm[extreme_idx, labels.index(\"Low\")] + cm[extreme_idx, labels.index(\"Medium\")]\n",
        "\n",
        "severe_miss_rate = missed_extreme / true_extreme\n",
        "severe_detection_rate = 1 - severe_miss_rate\n",
        "\n",
        "print(\"\\nSevere Congestion Detection Rate:\", round(severe_detection_rate,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTmNlWVpDrVD",
        "outputId": "9eb1edd2-8cdb-458f-c0b0-f4eddee6785d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Severe Congestion Detection Rate: 0.9273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# catastrophic error rate\n",
        "catastrophic = 0\n",
        "total = cm.sum()\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    for j in range(len(labels)):\n",
        "        if abs(i-j) >= 2:\n",
        "            catastrophic += cm[i,j]\n",
        "\n",
        "print(\"Catastrophic Error Rate:\", round(catastrophic/total,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6n8lT6zDtpU",
        "outputId": "dabade54-7c60-43a9-b74d-e0685d64453b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Catastrophic Error Rate: 0.0031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "kappa = cohen_kappa_score(y_true, y_pred)\n",
        "print(\"Cohen Kappa:\", round(kappa,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehlxfO4qDzhg",
        "outputId": "cb372913-e31a-4ff3-f487-fb8bf65ed1e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen Kappa: 0.7873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — Random Forest 30-Minute Crowd Forecast\n",
        "Regression + Classification Evaluation\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    cohen_kappa_score\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 1) FEATURE COLUMNS (نفس XGBoost)\n",
        "# ─────────────────────────────────────────\n",
        "FEATURES = [\n",
        "    \"hour\", \"minute_of_day\", \"day_of_week\", \"is_weekend\",\n",
        "    \"station_id\", \"headway_seconds\",\n",
        "    \"event_flag\", \"holiday_flag\", \"special_event_type\",\n",
        "    \"lag_5\", \"lag_15\", \"lag_30\", \"lag_60\", \"lag_120\",\n",
        "    \"roll_mean_15\", \"roll_std_15\", \"roll_mean_60\",\n",
        "]\n",
        "TARGET = \"target_30m\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 2) TRAIN / TEST SPLIT (نفس XGBoost بالضبط)\n",
        "# ─────────────────────────────────────────\n",
        "# df هو نفس الـ dataframe اللي استخدمتيه مع XGBoost\n",
        "df_sorted = df.sort_values([\"timestamp\", \"station_id\"]).reset_index(drop=True)\n",
        "\n",
        "X_sorted = df_sorted[FEATURES].copy()\n",
        "y_sorted = df_sorted[TARGET].copy()\n",
        "\n",
        "n = len(df_sorted)\n",
        "train_end = int(n * 0.70)\n",
        "val_end   = int(n * 0.85)\n",
        "\n",
        "X_train = X_sorted.iloc[:train_end]\n",
        "y_train = y_sorted.iloc[:train_end]\n",
        "\n",
        "X_val   = X_sorted.iloc[train_end:val_end]\n",
        "y_val   = y_sorted.iloc[train_end:val_end]\n",
        "\n",
        "X_test  = X_sorted.iloc[val_end:]\n",
        "y_test  = y_sorted.iloc[val_end:]\n",
        "\n",
        "print(f\"Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 3) TRAIN RANDOM FOREST REGRESSOR\n",
        "# ─────────────────────────────────────────\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=8,\n",
        "    min_samples_leaf=3,\n",
        "    max_features=\"sqrt\",\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 4) REGRESSION METRICS\n",
        "# ─────────────────────────────────────────\n",
        "y_pred_reg = rf_model.predict(X_test)\n",
        "y_true_reg = y_test.values\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
        "mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "r2   = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "print(\"\\n========== REGRESSION METRICS (RF) ==========\")\n",
        "print(f\"RMSE : {rmse:,.2f}\")\n",
        "print(f\"MAE  : {mae:,.2f}\")\n",
        "print(f\"R²   : {r2:.4f}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 5) تحويل من regression → congestion levels\n",
        "#    نفس cap=4800 اللي استخدمتيه مع XGBoost\n",
        "# ─────────────────────────────────────────\n",
        "CAP = 4800\n",
        "LABELS = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "def to_level(x, cap=CAP):\n",
        "    r = x / cap\n",
        "    if r < 0.40:   return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "y_true_cls = [to_level(v) for v in y_true_reg]\n",
        "y_pred_cls = [to_level(v) for v in y_pred_reg]\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 6) CLASSIFICATION METRICS\n",
        "# ─────────────────────────────────────────\n",
        "acc  = accuracy_score(y_true_cls, y_pred_cls)\n",
        "bacc = balanced_accuracy_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=\"macro\", zero_division=0)\n",
        "p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=\"weighted\", zero_division=0)\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=None, zero_division=0)\n",
        "\n",
        "per_class = pd.DataFrame({\n",
        "    \"Level\":     LABELS,\n",
        "    \"Precision\": np.round(p_cls, 2),\n",
        "    \"Recall\":    np.round(r_cls, 2),\n",
        "    \"F1\":        np.round(f_cls, 2),\n",
        "    \"Support\":   support\n",
        "})\n",
        "\n",
        "cm = confusion_matrix(y_true_cls, y_pred_cls, labels=LABELS)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "    index=[f\"True_{l}\" for l in LABELS],\n",
        "    columns=[f\"Pred_{l}\" for l in LABELS])\n",
        "\n",
        "print(\"\\n========== CLASSIFICATION METRICS (RF) ==========\")\n",
        "print(f\"Accuracy         : {round(acc, 4)}\")\n",
        "print(f\"Balanced Accuracy: {round(bacc, 4)}\")\n",
        "print(f\"\\nMacro  P/R/F1 : {round(p_macro,4)} {round(r_macro,4)} {round(f_macro,4)}\")\n",
        "print(f\"Weighted P/R/F1: {round(p_weight,4)} {round(r_weight,4)} {round(f_weight,4)}\")\n",
        "print(\"\\nPer-class table:\")\n",
        "print(per_class.to_string(index=False))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 7) CUSTOM OPERATIONAL METRICS\n",
        "# ─────────────────────────────────────────\n",
        "extreme_idx = LABELS.index(\"Extreme\")\n",
        "true_extreme = cm[extreme_idx].sum()\n",
        "missed_extreme = (cm[extreme_idx, LABELS.index(\"Low\")] +\n",
        "                  cm[extreme_idx, LABELS.index(\"Medium\")])\n",
        "severe_detection_rate = 1 - (missed_extreme / true_extreme)\n",
        "\n",
        "catastrophic = sum(\n",
        "    cm[i, j]\n",
        "    for i in range(len(LABELS))\n",
        "    for j in range(len(LABELS))\n",
        "    if abs(i - j) >= 2\n",
        ")\n",
        "catastrophic_rate = catastrophic / cm.sum()\n",
        "\n",
        "kappa = cohen_kappa_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "print(\"\\n========== CUSTOM METRICS (RF) ==========\")\n",
        "print(f\"Severe Congestion Detection Rate : {round(severe_detection_rate, 4)}\")\n",
        "print(f\"Catastrophic Error Rate          : {round(catastrophic_rate, 4)}\")\n",
        "print(f\"Cohen's Kappa                    : {round(kappa, 4)}\")"
      ],
      "metadata": {
        "id": "az3EN5ylrbMy",
        "outputId": "f2b18577-691a-4d16-8cb4-8c1e9b389ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (136080, 17) | Val: (29160, 17) | Test: (29160, 17)\n",
            "Training finished.\n",
            "\n",
            "========== REGRESSION METRICS (RF) ==========\n",
            "RMSE : 364.37\n",
            "MAE  : 186.79\n",
            "R²   : 0.9225\n",
            "\n",
            "========== CLASSIFICATION METRICS (RF) ==========\n",
            "Accuracy         : 0.9025\n",
            "Balanced Accuracy: 0.7214\n",
            "\n",
            "Macro  P/R/F1 : 0.767 0.7214 0.7415\n",
            "Weighted P/R/F1: 0.898 0.9025 0.8998\n",
            "\n",
            "Per-class table:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.96    0.98 0.97    19825\n",
            " Medium       0.83    0.83 0.83     6272\n",
            "   High       0.59    0.53 0.56     1949\n",
            "Extreme       0.69    0.54 0.60     1114\n",
            "\n",
            "Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         19451          374          0             0\n",
            "True_Medium        756         5229        285             2\n",
            "True_High            1          638       1039           271\n",
            "True_Extreme         0           83        432           599\n",
            "\n",
            "========== CUSTOM METRICS (RF) ==========\n",
            "Severe Congestion Detection Rate : 0.9255\n",
            "Catastrophic Error Rate          : 0.0029\n",
            "Cohen's Kappa                    : 0.7957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — Fast XGBoost Tuning with Optuna\n",
        "أسرع من GridSearch بـ 10x\n",
        "\"\"\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 0) تثبيت Optuna\n",
        "# ─────────────────────────────────────────\n",
        "!pip install optuna -q\n",
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 1) OBJECTIVE FUNCTION\n",
        "# ─────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\":      trial.suggest_int(\"n_estimators\", 200, 600),\n",
        "        \"learning_rate\":     trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
        "        \"max_depth\":         trial.suggest_int(\"max_depth\", 3, 9),\n",
        "        \"subsample\":         trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\":  trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"reg_alpha\":         trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
        "        \"reg_lambda\":        trial.suggest_float(\"reg_lambda\", 0.5, 3.0),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "    y_pred = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    return rmse\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 2) RUN STUDY — 40 trials (~5-10 دقائق)\n",
        "# ─────────────────────────────────────────\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=40, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n✅ Best RMSE (Val):\", round(study.best_value, 2))\n",
        "print(\"✅ Best Params:\")\n",
        "for k, v in study.best_params.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 3) TRAIN BEST MODEL\n",
        "# ─────────────────────────────────────────\n",
        "best_params = study.best_params\n",
        "best_params[\"random_state\"] = 42\n",
        "best_params[\"n_jobs\"] = -1\n",
        "\n",
        "best_xgb = XGBRegressor(**best_params)\n",
        "best_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "print(\"\\nBest XGBoost training finished.\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 4) EVALUATE — نفس دالة evaluate من الكود السابق\n",
        "# ─────────────────────────────────────────\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "xgb_results = evaluate(\"XGBoost Tuned (Proposed)\", y_test.values, y_pred_xgb, station_ids_test)\n"
      ],
      "metadata": {
        "id": "QpjGEHUqvwgP",
        "outputId": "ba8d642a-216d-41fd-b39b-75c21ea14a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847,
          "referenced_widgets": [
            "5b3370be8b9440a2ba25831dfaf93bd7",
            "06f44f6bfdd04351a1eb17fab93b49f8",
            "6b22a07051274b1081aaeef7685a9f52",
            "68b9b6d285d641cba8f265ebe0cc5f01",
            "132edfd2aa8c456abc60ebfe6d62d364",
            "f8847f1153e249cbae69ce0a67fd7c04",
            "48e85cdbb8da43f189fac8e36ce26023",
            "0fb51b4f982a4a1ba763ed090908854c",
            "f7e55f7b89c74933b0fc3a327818bfd4",
            "f26e3dc073c547d6bde3141a966f15fc",
            "b522c2f6c19c493db7c457ec816f593b"
          ]
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/413.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m327.7/413.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b3370be8b9440a2ba25831dfaf93bd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Best RMSE (Val): 714.43\n",
            "✅ Best Params:\n",
            "   n_estimators: 567\n",
            "   learning_rate: 0.12209116662504584\n",
            "   max_depth: 4\n",
            "   subsample: 0.9590132644684392\n",
            "   colsample_bytree: 0.6035744908379123\n",
            "   reg_alpha: 0.7245893807030286\n",
            "   reg_lambda: 0.742577876300213\n",
            "\n",
            "Best XGBoost training finished.\n",
            "\n",
            "=======================================================\n",
            "  REGRESSION METRICS — XGBoost Tuned (Proposed)\n",
            "=======================================================\n",
            "  RMSE : 363.42\n",
            "  MAE  : 197.52\n",
            "  R²   : 0.9229\n",
            "\n",
            "=======================================================\n",
            "  CLASSIFICATION METRICS — XGBoost Tuned (Proposed)\n",
            "=======================================================\n",
            "  Accuracy          : 0.8595\n",
            "  Balanced Accuracy : 0.7999\n",
            "  Macro  P/R/F1     : 0.7935 0.7999 0.7911\n",
            "  Weighted P/R/F1   : 0.8602 0.8595 0.8568\n",
            "\n",
            "  Per-class:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.96    0.95 0.96    13875\n",
            " Medium       0.86    0.86 0.86     7564\n",
            "   High       0.67    0.51 0.58     3955\n",
            "Extreme       0.68    0.88 0.77     3766\n",
            "\n",
            "  Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         13238          620         16             1\n",
            "True_Medium        481         6514        529            40\n",
            "True_High            0          460       2014          1481\n",
            "True_Extreme         0           21        449          3296\n",
            "\n",
            "  Severe Detection Rate : 0.9944\n",
            "  Catastrophic Error    : 0.0027\n",
            "  Cohen's Kappa         : 0.7912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — XGBoost vs Random Forest\n",
        "Per-Station Capacity + Full Evaluation\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    cohen_kappa_score\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 1) STATION CAPACITY MAP (حقيقي)\n",
        "# ─────────────────────────────────────────\n",
        "STATION_CAPACITY = {\n",
        "    1: 4800,  # S1 - KAFD\n",
        "    2: 3200,  # S2 - STC\n",
        "    3: 4200,  # S3 - QASR\n",
        "    4: 2800,  # S4 - MUSEUM\n",
        "    5: 3800,  # S5 - AIRPORT\n",
        "    6: 1600,  # S6 - Western\n",
        "}\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 2) FEATURES & TARGET\n",
        "# ─────────────────────────────────────────\n",
        "FEATURES = [\n",
        "    \"hour\", \"minute_of_day\", \"day_of_week\", \"is_weekend\",\n",
        "    \"station_id\", \"headway_seconds\",\n",
        "    \"event_flag\", \"holiday_flag\", \"special_event_type\",\n",
        "    \"lag_5\", \"lag_15\", \"lag_30\", \"lag_60\", \"lag_120\",\n",
        "    \"roll_mean_15\", \"roll_std_15\", \"roll_mean_60\",\n",
        "]\n",
        "TARGET = \"target_30m\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 3) SPLIT\n",
        "# ─────────────────────────────────────────\n",
        "df_sorted = df.sort_values([\"timestamp\", \"station_id\"]).reset_index(drop=True)\n",
        "\n",
        "X_sorted = df_sorted[FEATURES].copy()\n",
        "y_sorted = df_sorted[TARGET].copy()\n",
        "\n",
        "n         = len(df_sorted)\n",
        "train_end = int(n * 0.70)\n",
        "val_end   = int(n * 0.85)\n",
        "\n",
        "X_train = X_sorted.iloc[:train_end]\n",
        "y_train = y_sorted.iloc[:train_end]\n",
        "X_val   = X_sorted.iloc[train_end:val_end]\n",
        "y_val   = y_sorted.iloc[train_end:val_end]\n",
        "X_test  = X_sorted.iloc[val_end:]\n",
        "y_test  = y_sorted.iloc[val_end:]\n",
        "\n",
        "# station_id للـ test (لاحتساب الـ capacity لكل صف)\n",
        "station_ids_test = df_sorted[\"station_id\"].iloc[val_end:].values\n",
        "\n",
        "print(f\"Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 4) تحويل per-station → congestion level\n",
        "# ─────────────────────────────────────────\n",
        "LABELS = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "def to_level_per_station(value, station_id):\n",
        "    cap = STATION_CAPACITY.get(int(station_id), 4800)\n",
        "    r = value / cap\n",
        "    if r < 0.40:   return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "def get_cls_labels(y_values, station_ids):\n",
        "    return [to_level_per_station(v, s) for v, s in zip(y_values, station_ids)]\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 5) EVALUATION FUNCTION\n",
        "# ─────────────────────────────────────────\n",
        "def evaluate(name, y_true_reg, y_pred_reg, station_ids):\n",
        "    # Regression\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
        "    mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "    r2   = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\"  REGRESSION METRICS — {name}\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"  RMSE : {rmse:,.2f}\")\n",
        "    print(f\"  MAE  : {mae:,.2f}\")\n",
        "    print(f\"  R²   : {r2:.4f}\")\n",
        "\n",
        "    # Classification\n",
        "    y_true_cls = get_cls_labels(y_true_reg, station_ids)\n",
        "    y_pred_cls = get_cls_labels(y_pred_reg, station_ids)\n",
        "\n",
        "    acc  = accuracy_score(y_true_cls, y_pred_cls)\n",
        "    bacc = balanced_accuracy_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "    p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=\"macro\", zero_division=0)\n",
        "    p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=\"weighted\", zero_division=0)\n",
        "    p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=None, zero_division=0)\n",
        "\n",
        "    per_class = pd.DataFrame({\n",
        "        \"Level\":     LABELS,\n",
        "        \"Precision\": np.round(p_cls, 2),\n",
        "        \"Recall\":    np.round(r_cls, 2),\n",
        "        \"F1\":        np.round(f_cls, 2),\n",
        "        \"Support\":   support\n",
        "    })\n",
        "\n",
        "    cm = confusion_matrix(y_true_cls, y_pred_cls, labels=LABELS)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "        index=[f\"True_{l}\" for l in LABELS],\n",
        "        columns=[f\"Pred_{l}\" for l in LABELS])\n",
        "\n",
        "    # Custom metrics\n",
        "    extreme_idx    = LABELS.index(\"Extreme\")\n",
        "    true_extreme   = cm[extreme_idx].sum()\n",
        "    missed_extreme = (cm[extreme_idx, LABELS.index(\"Low\")] +\n",
        "                      cm[extreme_idx, LABELS.index(\"Medium\")])\n",
        "    severe_det     = 1 - (missed_extreme / true_extreme) if true_extreme > 0 else 0\n",
        "\n",
        "    catastrophic = sum(\n",
        "        cm[i, j]\n",
        "        for i in range(len(LABELS))\n",
        "        for j in range(len(LABELS))\n",
        "        if abs(i - j) >= 2\n",
        "    )\n",
        "    cat_rate = catastrophic / cm.sum()\n",
        "    kappa    = cohen_kappa_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\"  CLASSIFICATION METRICS — {name}\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"  Accuracy          : {round(acc, 4)}\")\n",
        "    print(f\"  Balanced Accuracy : {round(bacc, 4)}\")\n",
        "    print(f\"  Macro  P/R/F1     : {round(p_macro,4)} {round(r_macro,4)} {round(f_macro,4)}\")\n",
        "    print(f\"  Weighted P/R/F1   : {round(p_weight,4)} {round(r_weight,4)} {round(f_weight,4)}\")\n",
        "    print(f\"\\n  Per-class:\\n{per_class.to_string(index=False)}\")\n",
        "    print(f\"\\n  Confusion Matrix:\\n{cm_df}\")\n",
        "    print(f\"\\n  Severe Detection Rate : {round(severe_det, 4)}\")\n",
        "    print(f\"  Catastrophic Error    : {round(cat_rate, 4)}\")\n",
        "    print(f\"  Cohen's Kappa         : {round(kappa, 4)}\")\n",
        "\n",
        "    return {\n",
        "        \"rmse\": rmse, \"mae\": mae, \"r2\": r2,\n",
        "        \"acc\": acc, \"bacc\": bacc,\n",
        "        \"macro_f1\": f_macro, \"weighted_f1\": f_weight,\n",
        "        \"severe_det\": severe_det, \"cat_rate\": cat_rate, \"kappa\": kappa\n",
        "    }\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 6) TRAIN & EVALUATE XGBOOST\n",
        "# ─────────────────────────────────────────\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.07,\n",
        "    max_depth=5,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "print(\"\\nXGBoost training finished.\")\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "xgb_results = evaluate(\"XGBoost (Proposed)\", y_test.values, y_pred_xgb, station_ids_test)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 7) TRAIN & EVALUATE RANDOM FOREST\n",
        "# ─────────────────────────────────────────\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=8,\n",
        "    min_samples_leaf=3,\n",
        "    max_features=\"sqrt\",\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"\\nRandom Forest training finished.\")\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rf_results = evaluate(\"Random Forest (Baseline)\", y_test.values, y_pred_rf, station_ids_test)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 8) SUMMARY TABLE\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"  FINAL COMPARISON SUMMARY\")\n",
        "print(\"=\"*55)\n",
        "metrics = [\"rmse\",\"mae\",\"r2\",\"acc\",\"bacc\",\"macro_f1\",\"weighted_f1\",\"severe_det\",\"cat_rate\",\"kappa\"]\n",
        "labels_print = [\"RMSE\",\"MAE\",\"R²\",\"Accuracy\",\"Balanced Acc\",\"Macro F1\",\"Weighted F1\",\"Severe Det\",\"Cat Error\",\"Kappa\"]\n",
        "\n",
        "for m, l in zip(metrics, labels_print):\n",
        "    print(f\"  {l:<18}: XGB={round(xgb_results[m],4)}  |  RF={round(rf_results[m],4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMSJS-cIt5de",
        "outputId": "a22ba896-9436-42ed-e590-bd04aebaf64f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (136080, 17) | Val: (29160, 17) | Test: (29160, 17)\n",
            "\n",
            "XGBoost training finished.\n",
            "\n",
            "=======================================================\n",
            "  REGRESSION METRICS — XGBoost (Proposed)\n",
            "=======================================================\n",
            "  RMSE : 376.59\n",
            "  MAE  : 202.33\n",
            "  R²   : 0.9172\n",
            "\n",
            "=======================================================\n",
            "  CLASSIFICATION METRICS — XGBoost (Proposed)\n",
            "=======================================================\n",
            "  Accuracy          : 0.8597\n",
            "  Balanced Accuracy : 0.7985\n",
            "  Macro  P/R/F1     : 0.7933 0.7985 0.7915\n",
            "  Weighted P/R/F1   : 0.8601 0.8597 0.8575\n",
            "\n",
            "  Per-class:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.96    0.96 0.96    13875\n",
            " Medium       0.86    0.85 0.86     7564\n",
            "   High       0.66    0.53 0.59     3955\n",
            "Extreme       0.68    0.86 0.76     3766\n",
            "\n",
            "  Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         13317          537          7            14\n",
            "True_Medium        528         6449        531            56\n",
            "True_High            0          451       2081          1423\n",
            "True_Extreme         0           30        514          3222\n",
            "\n",
            "  Severe Detection Rate : 0.992\n",
            "  Catastrophic Error    : 0.0037\n",
            "  Cohen's Kappa         : 0.7913\n",
            "\n",
            "Random Forest training finished.\n",
            "\n",
            "=======================================================\n",
            "  REGRESSION METRICS — Random Forest (Baseline)\n",
            "=======================================================\n",
            "  RMSE : 364.37\n",
            "  MAE  : 186.79\n",
            "  R²   : 0.9225\n",
            "\n",
            "=======================================================\n",
            "  CLASSIFICATION METRICS — Random Forest (Baseline)\n",
            "=======================================================\n",
            "  Accuracy          : 0.8753\n",
            "  Balanced Accuracy : 0.8216\n",
            "  Macro  P/R/F1     : 0.8162 0.8216 0.8171\n",
            "  Weighted P/R/F1   : 0.876 0.8753 0.8747\n",
            "\n",
            "  Per-class:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.97    0.96 0.96    13875\n",
            " Medium       0.88    0.88 0.88     7564\n",
            "   High       0.70    0.61 0.65     3955\n",
            "Extreme       0.72    0.83 0.77     3766\n",
            "\n",
            "  Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         13292          558          9            16\n",
            "True_Medium        431         6664        427            42\n",
            "True_High            0          364       2424          1167\n",
            "True_Extreme         0           28        595          3143\n",
            "\n",
            "  Severe Detection Rate : 0.9926\n",
            "  Catastrophic Error    : 0.0033\n",
            "  Cohen's Kappa         : 0.8147\n",
            "\n",
            "=======================================================\n",
            "  FINAL COMPARISON SUMMARY\n",
            "=======================================================\n",
            "  RMSE              : XGB=376.5906  |  RF=364.3732\n",
            "  MAE               : XGB=202.3259  |  RF=186.7856\n",
            "  R²                : XGB=0.9172  |  RF=0.9225\n",
            "  Accuracy          : XGB=0.8597  |  RF=0.8753\n",
            "  Balanced Acc      : XGB=0.7985  |  RF=0.8216\n",
            "  Macro F1          : XGB=0.7915  |  RF=0.8171\n",
            "  Weighted F1       : XGB=0.8575  |  RF=0.8747\n",
            "  Severe Det        : XGB=0.992  |  RF=0.9926\n",
            "  Cat Error         : XGB=0.0037  |  RF=0.0033\n",
            "  Kappa             : XGB=0.7913  |  RF=0.8147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — LSTM & SARIMA Evaluation\n",
        "Per-Station Capacity — نفس الـ split والـ capacity\n",
        "شغّلي هذا الكود بعد كود XGBoost/RF\n",
        "\"\"\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 0) INSTALL\n",
        "# ─────────────────────────────────────────\n",
        "!pip install tensorflow -q\n",
        "!pip install statsmodels -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    cohen_kappa_score, mean_squared_error,\n",
        "    mean_absolute_error, r2_score\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# STATION CAPACITY & LABELS\n",
        "# ─────────────────────────────────────────\n",
        "STATION_CAPACITY = {\n",
        "    1: 4800, 2: 3200, 3: 4200,\n",
        "    4: 2800, 5: 3800, 6: 1600,\n",
        "}\n",
        "LABELS = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "def to_level_per_station(value, station_id):\n",
        "    cap = STATION_CAPACITY.get(int(station_id), 4800)\n",
        "    r = value / cap\n",
        "    if r < 0.40:   return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "def get_cls_labels(y_values, station_ids):\n",
        "    return [to_level_per_station(v, s) for v, s in zip(y_values, station_ids)]\n",
        "\n",
        "def print_metrics(name, y_true_reg, y_pred_reg, station_ids):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
        "    mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "    r2   = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "    y_true_cls = get_cls_labels(y_true_reg, station_ids)\n",
        "    y_pred_cls = get_cls_labels(y_pred_reg, station_ids)\n",
        "\n",
        "    acc  = accuracy_score(y_true_cls, y_pred_cls)\n",
        "    bacc = balanced_accuracy_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "    p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=\"macro\", zero_division=0)\n",
        "    p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=\"weighted\", zero_division=0)\n",
        "    p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=None, zero_division=0)\n",
        "\n",
        "    cm = confusion_matrix(y_true_cls, y_pred_cls, labels=LABELS)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "        index=[f\"True_{l}\" for l in LABELS],\n",
        "        columns=[f\"Pred_{l}\" for l in LABELS])\n",
        "\n",
        "    extreme_idx    = LABELS.index(\"Extreme\")\n",
        "    true_extreme   = cm[extreme_idx].sum()\n",
        "    missed_extreme = (cm[extreme_idx, LABELS.index(\"Low\")] +\n",
        "                      cm[extreme_idx, LABELS.index(\"Medium\")])\n",
        "    severe_det  = 1 - (missed_extreme / true_extreme) if true_extreme > 0 else 0\n",
        "    cat_rate    = sum(cm[i,j] for i in range(4) for j in range(4) if abs(i-j)>=2) / cm.sum()\n",
        "    kappa       = cohen_kappa_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\"  {name}\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"  RMSE              : {rmse:,.2f}\")\n",
        "    print(f\"  MAE               : {mae:,.2f}\")\n",
        "    print(f\"  R²                : {r2:.4f}\")\n",
        "    print(f\"  Accuracy          : {round(acc,4)}\")\n",
        "    print(f\"  Balanced Accuracy : {round(bacc,4)}\")\n",
        "    print(f\"  Macro F1          : {round(f_macro,4)}\")\n",
        "    print(f\"  Weighted F1       : {round(f_weight,4)}\")\n",
        "    print(f\"  Severe Detection  : {round(severe_det,4)}\")\n",
        "    print(f\"  Cat Error         : {round(cat_rate,4)}\")\n",
        "    print(f\"  Cohen's Kappa     : {round(kappa,4)}\")\n",
        "    print(f\"\\n  Confusion Matrix:\\n{cm_df}\")\n",
        "\n",
        "    return {\n",
        "        \"rmse\":rmse,\"mae\":mae,\"r2\":r2,\n",
        "        \"acc\":acc,\"bacc\":bacc,\n",
        "        \"macro_f1\":f_macro,\"weighted_f1\":f_weight,\n",
        "        \"severe_det\":severe_det,\"cat_rate\":cat_rate,\"kappa\":kappa\n",
        "    }\n",
        "\n",
        "# ══════════════════════════════════════════════════════\n",
        "# PART 1 — LSTM\n",
        "# ══════════════════════════════════════════════════════\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"  TRAINING LSTM\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_sc = scaler.fit_transform(X_train)\n",
        "X_val_sc   = scaler.transform(X_val)\n",
        "X_test_sc  = scaler.transform(X_test)\n",
        "\n",
        "# Reshape for LSTM: (samples, timesteps=1, features)\n",
        "X_train_lstm = X_train_sc.reshape(X_train_sc.shape[0], 1, X_train_sc.shape[1])\n",
        "X_val_lstm   = X_val_sc.reshape(X_val_sc.shape[0],   1, X_val_sc.shape[1])\n",
        "X_test_lstm  = X_test_sc.reshape(X_test_sc.shape[0],  1, X_test_sc.shape[1])\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(1, X_train_sc.shape[1]), return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "lstm_model.fit(\n",
        "    X_train_lstm, y_train.values,\n",
        "    validation_data=(X_val_lstm, y_val.values),\n",
        "    epochs=30,\n",
        "    batch_size=512,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm).flatten()\n",
        "lstm_results = print_metrics(\"LSTM (Deep Learning Baseline)\",\n",
        "                              y_test.values, y_pred_lstm, station_ids_test)\n",
        "\n",
        "# ══════════════════════════════════════════════════════\n",
        "# PART 2 — SARIMA (per station, one station sample)\n",
        "# ══════════════════════════════════════════════════════\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"  TRAINING SARIMA (per station)\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# SARIMA على كل محطة على حدة ثم نجمع النتائج\n",
        "df_sorted_copy = df_sorted.copy()\n",
        "\n",
        "# نحدد test timestamps\n",
        "test_start_idx = val_end\n",
        "all_true  = []\n",
        "all_pred  = []\n",
        "all_sids  = []\n",
        "\n",
        "for sid in [1, 2, 3, 4, 5, 6]:\n",
        "    print(f\"\\n  Fitting SARIMA for Station {sid}...\")\n",
        "\n",
        "    # فلتر البيانات لهذه المحطة\n",
        "    mask    = df_sorted_copy[\"station_id\"] == sid\n",
        "    df_s    = df_sorted_copy[mask].reset_index(drop=True)\n",
        "    y_s     = df_s[\"target_30m\"].values\n",
        "\n",
        "    n_s        = len(df_s)\n",
        "    train_e    = int(n_s * 0.70)\n",
        "    val_e      = int(n_s * 0.85)\n",
        "\n",
        "    y_train_s  = y_s[:train_e]\n",
        "    y_test_s   = y_s[val_e:]\n",
        "\n",
        "    try:\n",
        "        # SARIMA(1,1,1)(1,1,1,24) — نمط يومي\n",
        "        model = SARIMAX(\n",
        "            y_train_s,\n",
        "            order=(1, 1, 1),\n",
        "            seasonal_order=(1, 1, 1, 24),\n",
        "            enforce_stationarity=False,\n",
        "            enforce_invertibility=False\n",
        "        )\n",
        "        result  = model.fit(disp=False, maxiter=50)\n",
        "        n_test  = len(y_test_s)\n",
        "        forecast = result.forecast(steps=n_test)\n",
        "\n",
        "        all_true.extend(y_test_s)\n",
        "        all_pred.extend(forecast)\n",
        "        all_sids.extend([sid] * n_test)\n",
        "        print(f\"  Station {sid} done. Test samples: {n_test}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Station {sid} SARIMA failed: {e}\")\n",
        "        # Fallback: persistence\n",
        "        fallback = np.full(len(y_test_s), y_train_s[-1])\n",
        "        all_true.extend(y_test_s)\n",
        "        all_pred.extend(fallback)\n",
        "        all_sids.extend([sid] * len(y_test_s))\n",
        "\n",
        "sarima_results = print_metrics(\n",
        "    \"SARIMA (Statistical Baseline)\",\n",
        "    np.array(all_true),\n",
        "    np.array(all_pred),\n",
        "    np.array(all_sids)\n",
        ")\n",
        "\n",
        "# ══════════════════════════════════════════════════════\n",
        "# FINAL SUMMARY — كل النماذج\n",
        "# ══════════════════════════════════════════════════════\n",
        "print(\"\\n\\n\" + \"=\"*65)\n",
        "print(\"  COMPLETE MODEL COMPARISON\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "all_results = {\n",
        "    \"XGBoost (Proposed)\":   xgb_results,\n",
        "    \"Random Forest\":         rf_results,\n",
        "    \"LSTM\":                  lstm_results,\n",
        "    \"SARIMA\":                sarima_results,\n",
        "}\n",
        "\n",
        "metrics_map = [\n",
        "    (\"rmse\",       \"RMSE ↓\"),\n",
        "    (\"mae\",        \"MAE ↓\"),\n",
        "    (\"r2\",         \"R² ↑\"),\n",
        "    (\"acc\",        \"Accuracy ↑\"),\n",
        "    (\"bacc\",       \"Balanced Acc ↑\"),\n",
        "    (\"macro_f1\",   \"Macro F1 ↑\"),\n",
        "    (\"weighted_f1\",\"Weighted F1 ↑\"),\n",
        "    (\"severe_det\", \"Severe Detection ↑\"),\n",
        "    (\"cat_rate\",   \"Cat Error ↓\"),\n",
        "    (\"kappa\",      \"Cohen's Kappa ↑\"),\n",
        "]\n",
        "\n",
        "header = f\"{'Metric':<22}\" + \"\".join(f\"{k:<22}\" for k in all_results.keys())\n",
        "print(header)\n",
        "print(\"-\" * (22 + 22*4))\n",
        "\n",
        "for key, label in metrics_map:\n",
        "    row = f\"{label:<22}\"\n",
        "    for res in all_results.values():\n",
        "        row += f\"{round(res[key],4):<22}\"\n",
        "    print(row)\n"
      ],
      "metadata": {
        "id": "uuO3y7gXxc1B",
        "outputId": "59749ef7-525d-4699-bc26-ec812cb8eaf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "  TRAINING LSTM\n",
            "=======================================================\n",
            "Epoch 1/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 4038640.0000 - val_loss: 6066078.0000\n",
            "Epoch 2/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 3388783.7500 - val_loss: 4459743.5000\n",
            "Epoch 3/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2125578.0000 - val_loss: 2973149.5000\n",
            "Epoch 4/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1196769.1250 - val_loss: 2176586.0000\n",
            "Epoch 5/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 775555.5000 - val_loss: 1779754.7500\n",
            "Epoch 6/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 602354.1250 - val_loss: 1612756.6250\n",
            "Epoch 7/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 540919.8750 - val_loss: 1510721.3750\n",
            "Epoch 8/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 502590.1562 - val_loss: 1450978.6250\n",
            "Epoch 9/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 467676.8750 - val_loss: 1381168.3750\n",
            "Epoch 10/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 437528.4688 - val_loss: 1324690.3750\n",
            "Epoch 11/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 423290.7500 - val_loss: 1269234.3750\n",
            "Epoch 12/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 399965.6562 - val_loss: 1234686.3750\n",
            "Epoch 13/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 390653.7188 - val_loss: 1203342.1250\n",
            "Epoch 14/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 375253.3125 - val_loss: 1149812.3750\n",
            "Epoch 15/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 363029.7500 - val_loss: 1128398.6250\n",
            "Epoch 16/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 345491.2500 - val_loss: 1096895.3750\n",
            "Epoch 17/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 332406.1562 - val_loss: 1075668.1250\n",
            "Epoch 18/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 322553.9375 - val_loss: 1044085.1250\n",
            "Epoch 19/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 319181.7500 - val_loss: 1013826.1875\n",
            "Epoch 20/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 306640.7188 - val_loss: 1003684.3750\n",
            "Epoch 21/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 298811.3125 - val_loss: 973959.1250\n",
            "Epoch 22/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 292440.8438 - val_loss: 954599.0625\n",
            "Epoch 23/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 287337.3438 - val_loss: 954623.6250\n",
            "Epoch 24/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 278753.8750 - val_loss: 938032.9375\n",
            "Epoch 25/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 276429.0000 - val_loss: 925289.8125\n",
            "Epoch 26/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 272216.9688 - val_loss: 926780.8750\n",
            "Epoch 27/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 264764.6562 - val_loss: 937702.6875\n",
            "Epoch 28/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 260668.4844 - val_loss: 925828.9375\n",
            "Epoch 29/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 261627.1094 - val_loss: 916603.9375\n",
            "Epoch 30/30\n",
            "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 254616.5000 - val_loss: 909815.5000\n",
            "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n",
            "\n",
            "=======================================================\n",
            "  LSTM (Deep Learning Baseline)\n",
            "=======================================================\n",
            "  RMSE              : 482.34\n",
            "  MAE               : 282.43\n",
            "  R²                : 0.8642\n",
            "  Accuracy          : 0.8096\n",
            "  Balanced Accuracy : 0.7438\n",
            "  Macro F1          : 0.7416\n",
            "  Weighted F1       : 0.8116\n",
            "  Severe Detection  : 0.9665\n",
            "  Cat Error         : 0.0079\n",
            "  Cohen's Kappa     : 0.7177\n",
            "\n",
            "  Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         12872          969         32             2\n",
            "True_Medium        704         5809        992            59\n",
            "True_High           11          673       2273           998\n",
            "True_Extreme         5          121        986          2654\n",
            "\n",
            "=======================================================\n",
            "  TRAINING SARIMA (per station)\n",
            "=======================================================\n",
            "\n",
            "  Fitting SARIMA for Station 1...\n",
            "  Station 1 done. Test samples: 4860\n",
            "\n",
            "  Fitting SARIMA for Station 2...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — Complete Evaluation: XGBoost + RF + LSTM + SARIMA\n",
        "Standalone — كل شيء في كود واحد\n",
        "\"\"\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 0) IMPORTS\n",
        "# ─────────────────────────────────────────\n",
        "!pip install optuna tensorflow statsmodels -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    cohen_kappa_score, mean_squared_error,\n",
        "    mean_absolute_error, r2_score\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 1) SETTINGS\n",
        "# ─────────────────────────────────────────\n",
        "STATION_CAPACITY = {\n",
        "    1: 4800, 2: 3200, 3: 4200,\n",
        "    4: 2800, 5: 3800, 6: 1600,\n",
        "}\n",
        "LABELS = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "FEATURES = [\n",
        "    \"hour\", \"minute_of_day\", \"day_of_week\", \"is_weekend\",\n",
        "    \"station_id\", \"headway_seconds\",\n",
        "    \"event_flag\", \"holiday_flag\", \"special_event_type\",\n",
        "    \"lag_5\", \"lag_15\", \"lag_30\", \"lag_60\", \"lag_120\",\n",
        "    \"roll_mean_15\", \"roll_std_15\", \"roll_mean_60\",\n",
        "]\n",
        "TARGET = \"target_30m\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 2) DATA SPLIT\n",
        "# ─────────────────────────────────────────\n",
        "df_sorted = df.sort_values([\"timestamp\", \"station_id\"]).reset_index(drop=True)\n",
        "\n",
        "n         = len(df_sorted)\n",
        "train_end = int(n * 0.70)\n",
        "val_end   = int(n * 0.85)\n",
        "\n",
        "X_sorted = df_sorted[FEATURES].copy()\n",
        "y_sorted = df_sorted[TARGET].copy()\n",
        "\n",
        "X_train = X_sorted.iloc[:train_end]\n",
        "y_train = y_sorted.iloc[:train_end]\n",
        "X_val   = X_sorted.iloc[train_end:val_end]\n",
        "y_val   = y_sorted.iloc[train_end:val_end]\n",
        "X_test  = X_sorted.iloc[val_end:]\n",
        "y_test  = y_sorted.iloc[val_end:]\n",
        "station_ids_test = df_sorted[\"station_id\"].iloc[val_end:].values\n",
        "\n",
        "print(f\"Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 3) HELPER FUNCTIONS\n",
        "# ─────────────────────────────────────────\n",
        "def to_level(value, station_id):\n",
        "    cap = STATION_CAPACITY.get(int(station_id), 4800)\n",
        "    r = value / cap\n",
        "    if r < 0.40:   return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "def get_cls(y_vals, sids):\n",
        "    return [to_level(v, s) for v, s in zip(y_vals, sids)]\n",
        "\n",
        "def compute_metrics(name, y_true_reg, y_pred_reg, sids):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
        "    mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "    r2   = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "    y_true_cls = get_cls(y_true_reg, sids)\n",
        "    y_pred_cls = get_cls(y_pred_reg, sids)\n",
        "\n",
        "    acc  = accuracy_score(y_true_cls, y_pred_cls)\n",
        "    bacc = balanced_accuracy_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "    _, _, f_macro,  _ = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=\"macro\",    zero_division=0)\n",
        "    _, _, f_weight, _ = precision_recall_fscore_support(\n",
        "        y_true_cls, y_pred_cls, labels=LABELS, average=\"weighted\",  zero_division=0)\n",
        "\n",
        "    cm = confusion_matrix(y_true_cls, y_pred_cls, labels=LABELS)\n",
        "\n",
        "    ei   = LABELS.index(\"Extreme\")\n",
        "    te   = cm[ei].sum()\n",
        "    me   = cm[ei, LABELS.index(\"Low\")] + cm[ei, LABELS.index(\"Medium\")]\n",
        "    sdr  = 1 - (me / te) if te > 0 else 0\n",
        "    cat  = sum(cm[i,j] for i in range(4) for j in range(4) if abs(i-j)>=2) / cm.sum()\n",
        "    kap  = cohen_kappa_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "    print(f\"\\n{'='*50}  {name}\")\n",
        "    print(f\"  RMSE={rmse:,.2f}  MAE={mae:,.2f}  R²={r2:.4f}\")\n",
        "    print(f\"  Acc={acc:.4f}  BalAcc={bacc:.4f}  MacroF1={f_macro:.4f}\")\n",
        "    print(f\"  SevereDetection={sdr:.4f}  CatError={cat:.4f}  Kappa={kap:.4f}\")\n",
        "\n",
        "    return {\"rmse\":rmse,\"mae\":mae,\"r2\":r2,\"acc\":acc,\"bacc\":bacc,\n",
        "            \"macro_f1\":f_macro,\"weighted_f1\":f_weight,\n",
        "            \"severe_det\":sdr,\"cat_rate\":cat,\"kappa\":kap}\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 4) XGBOOST\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\n>>> Training XGBoost...\")\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=567, learning_rate=0.122, max_depth=4,\n",
        "    subsample=0.959, colsample_bytree=0.604,\n",
        "    reg_alpha=0.725, reg_lambda=0.743,\n",
        "    random_state=42, n_jobs=-1\n",
        ")\n",
        "xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "xgb_results = compute_metrics(\"XGBoost (Proposed)\",\n",
        "    y_test.values, xgb.predict(X_test), station_ids_test)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 5) RANDOM FOREST\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\n>>> Training Random Forest...\")\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300, max_depth=None,\n",
        "    min_samples_split=8, min_samples_leaf=3,\n",
        "    max_features=\"sqrt\", random_state=42, n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_results = compute_metrics(\"Random Forest (Baseline)\",\n",
        "    y_test.values, rf.predict(X_test), station_ids_test)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 6) LSTM\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\n>>> Training LSTM...\")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xtr_sc = scaler.fit_transform(X_train).reshape(-1, 1, len(FEATURES))\n",
        "Xva_sc = scaler.transform(X_val).reshape(-1, 1, len(FEATURES))\n",
        "Xte_sc = scaler.transform(X_test).reshape(-1, 1, len(FEATURES))\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(1, len(FEATURES))),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dropout(0.1),\n",
        "    Dense(1)\n",
        "])\n",
        "lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "lstm_model.fit(Xtr_sc, y_train.values,\n",
        "               validation_data=(Xva_sc, y_val.values),\n",
        "               epochs=30, batch_size=512,\n",
        "               callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
        "               verbose=0)\n",
        "\n",
        "lstm_results = compute_metrics(\"LSTM (Deep Learning Baseline)\",\n",
        "    y_test.values, lstm_model.predict(Xte_sc).flatten(), station_ids_test)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 7) SARIMA — الإصلاح الحقيقي\n",
        "#    نستخدم ARIMA على congestion score (0-1)\n",
        "#    بدل passenger count — أرقام أصغر وأكثر استقراراً\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\n>>> Training SARIMA...\")\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "all_true, all_pred, all_sids = [], [], []\n",
        "\n",
        "for sid in [1, 2, 3, 4, 5, 6]:\n",
        "    print(f\"  S{sid}...\", end=\" \")\n",
        "    cap   = STATION_CAPACITY[sid]\n",
        "    mask  = df_sorted[\"station_id\"] == sid\n",
        "    df_s  = df_sorted[mask].reset_index(drop=True)\n",
        "\n",
        "    # نحول لـ congestion score (0-1) — أكثر استقراراً لـ ARIMA\n",
        "    y_score = (df_s[TARGET] / cap).values\n",
        "    y_raw   = df_s[TARGET].values\n",
        "\n",
        "    n_s     = len(df_s)\n",
        "    train_e = int(n_s * 0.70)\n",
        "    val_e   = int(n_s * 0.85)\n",
        "\n",
        "    # آخر 1000 نقطة للتدريب\n",
        "    y_train_s = y_score[max(0, train_e-1000):train_e]\n",
        "    y_test_s  = y_raw[val_e:]\n",
        "    n_test    = len(y_test_s)\n",
        "\n",
        "    try:\n",
        "        model    = ARIMA(y_train_s, order=(2, 1, 2))\n",
        "        result   = model.fit()\n",
        "        forecast = result.forecast(steps=n_test)\n",
        "        # رجّع لـ passenger count\n",
        "        forecast_raw = np.clip(forecast * cap, 0, cap * 1.5)\n",
        "\n",
        "        all_true.extend(y_test_s)\n",
        "        all_pred.extend(forecast_raw)\n",
        "        all_sids.extend([sid] * n_test)\n",
        "        print(\"✅\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ {e}\")\n",
        "        # Fallback: historical mean per hour\n",
        "        fallback = np.full(n_test, np.mean(y_test_s) * 0.7)\n",
        "        all_true.extend(y_test_s)\n",
        "        all_pred.extend(fallback)\n",
        "        all_sids.extend([sid] * n_test)\n",
        "\n",
        "sarima_results = compute_metrics(\"SARIMA (Statistical Baseline)\",\n",
        "    np.array(all_true), np.array(all_pred), np.array(all_sids))\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 8) FINAL COMPARISON TABLE\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\n\\n\" + \"=\"*75)\n",
        "print(\"  COMPLETE MODEL COMPARISON — MASAR\")\n",
        "print(\"=\"*75)\n",
        "\n",
        "all_res = {\n",
        "    \"XGBoost\":  xgb_results,\n",
        "    \"RF\":        rf_results,\n",
        "    \"LSTM\":      lstm_results,\n",
        "    \"SARIMA\":    sarima_results,\n",
        "}\n",
        "\n",
        "metrics_map = [\n",
        "    (\"rmse\",        \"RMSE ↓\"),\n",
        "    (\"mae\",         \"MAE ↓\"),\n",
        "    (\"r2\",          \"R² ↑\"),\n",
        "    (\"acc\",         \"Accuracy ↑\"),\n",
        "    (\"bacc\",        \"Balanced Acc ↑\"),\n",
        "    (\"macro_f1\",    \"Macro F1 ↑\"),\n",
        "    (\"weighted_f1\", \"Weighted F1 ↑\"),\n",
        "    (\"severe_det\",  \"Severe Det ↑\"),\n",
        "    (\"cat_rate\",    \"Cat Error ↓\"),\n",
        "    (\"kappa\",       \"Kappa ↑\"),\n",
        "]\n",
        "\n",
        "header = f\"{'Metric':<22}\" + \"\".join(f\"{k:<18}\" for k in all_res.keys())\n",
        "print(header)\n",
        "print(\"-\" * (22 + 18*4))\n",
        "for key, label in metrics_map:\n",
        "    row = f\"{label:<22}\"\n",
        "    for res in all_res.values():\n",
        "        row += f\"{round(res[key],4):<18}\"\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "CW479KvV0Yrm",
        "outputId": "3cf0854c-0588-428a-dcfa-3dd8233941ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (136080, 17) | Val: (29160, 17) | Test: (29160, 17)\n",
            "\n",
            ">>> Training XGBoost...\n",
            "\n",
            "==================================================  XGBoost (Proposed)\n",
            "  RMSE=361.07  MAE=195.95  R²=0.9239\n",
            "  Acc=0.8607  BalAcc=0.7995  MacroF1=0.7909\n",
            "  SevereDetection=0.9947  CatError=0.0029  Kappa=0.7928\n",
            "\n",
            ">>> Training Random Forest...\n",
            "\n",
            "==================================================  Random Forest (Baseline)\n",
            "  RMSE=363.60  MAE=186.06  R²=0.9229\n",
            "  Acc=0.8754  BalAcc=0.8215  MacroF1=0.8172\n",
            "  SevereDetection=0.9915  CatError=0.0033  Kappa=0.8148\n",
            "\n",
            ">>> Training LSTM...\n",
            "\u001b[1m912/912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-850035109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m                verbose=0)\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m lstm_results = compute_metrics(\"LSTM (Deep Learning Baseline)\",\n\u001b[0m\u001b[1;32m    170\u001b[0m     y_test.values, lstm_model.predict(Xte_sc).flatten(), station_ids_test)\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-850035109.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(name, y_true_reg, y_pred_reg, sids)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mmae\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mr2\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[0;32m--> 565\u001b[0;31m         _check_reg_targets_with_floating_dtype(\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_matching_floating_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — SARIMA Rolling Forecast (Fixed)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    cohen_kappa_score, mean_squared_error,\n",
        "    mean_absolute_error, r2_score\n",
        ")\n",
        "\n",
        "STATION_CAPACITY = {\n",
        "    1: 4800, 2: 3200, 3: 4200,\n",
        "    4: 2800, 5: 3800, 6: 1600,\n",
        "}\n",
        "LABELS = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "TARGET = \"target_30m\"\n",
        "\n",
        "def to_level(value, station_id):\n",
        "    cap = STATION_CAPACITY.get(int(station_id), 4800)\n",
        "    r = value / cap\n",
        "    if r < 0.40:   return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "def get_cls(y_vals, sids):\n",
        "    return [to_level(v, s) for v, s in zip(y_vals, sids)]\n",
        "\n",
        "df_sorted = df.sort_values([\"timestamp\", \"station_id\"]).reset_index(drop=True)\n",
        "print(f\"Data ready: {df_sorted.shape}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# ROLLING FORECAST — نتوقع كل 30 دقيقة\n",
        "# ونضيف القيمة الحقيقية للـ history\n",
        "# ─────────────────────────────────────────\n",
        "print(\"\\nFitting ARIMA with rolling forecast...\")\n",
        "\n",
        "all_true, all_pred, all_sids = [], [], []\n",
        "\n",
        "# نأخذ عينة من كل محطة (كل 30 نقطة) لتسريع الكود\n",
        "STEP = 30  # نتوقع كل 30 دقيقة (مش كل دقيقة)\n",
        "\n",
        "for sid in [1, 2, 3, 4, 5, 6]:\n",
        "    print(f\"  S{sid}...\", end=\" \")\n",
        "\n",
        "    cap  = STATION_CAPACITY[sid]\n",
        "    mask = df_sorted[\"station_id\"] == sid\n",
        "    df_s = df_sorted[mask].reset_index(drop=True)\n",
        "\n",
        "    y_raw = df_s[TARGET].values\n",
        "    n_s   = len(df_s)\n",
        "    train_e = int(n_s * 0.70)\n",
        "    val_e   = int(n_s * 0.85)\n",
        "\n",
        "    # history: آخر 200 نقطة من train\n",
        "    history = list(y_raw[max(0, train_e-200):train_e])\n",
        "    y_test_s = y_raw[val_e:]\n",
        "\n",
        "    # نأخذ عينة كل STEP نقطة\n",
        "    test_indices = list(range(0, len(y_test_s), STEP))\n",
        "    n_sample = len(test_indices)\n",
        "\n",
        "    preds = []\n",
        "    trues = []\n",
        "\n",
        "    try:\n",
        "        for i in test_indices:\n",
        "            # train ARIMA على الـ history\n",
        "            model  = ARIMA(history[-200:], order=(1, 1, 1))\n",
        "            result = model.fit()\n",
        "            yhat   = result.forecast(steps=1)[0]\n",
        "            yhat   = np.clip(yhat, 0, cap * 1.5)\n",
        "\n",
        "            preds.append(yhat)\n",
        "            trues.append(y_test_s[i])\n",
        "\n",
        "            # أضف القيمة الحقيقية للـ history\n",
        "            history.append(y_test_s[i])\n",
        "\n",
        "        all_true.extend(trues)\n",
        "        all_pred.extend(preds)\n",
        "        all_sids.extend([sid] * len(trues))\n",
        "        print(f\"✅ ({n_sample} points)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ fallback — {e}\")\n",
        "        fallback = [np.mean(history)] * len(test_indices)\n",
        "        all_true.extend([y_test_s[i] for i in test_indices])\n",
        "        all_pred.extend(fallback)\n",
        "        all_sids.extend([sid] * len(test_indices))\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# METRICS\n",
        "# ─────────────────────────────────────────\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "all_sids = np.array(all_sids)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
        "mae  = mean_absolute_error(all_true, all_pred)\n",
        "r2   = r2_score(all_true, all_pred)\n",
        "\n",
        "y_true_cls = get_cls(all_true, all_sids)\n",
        "y_pred_cls = get_cls(all_pred, all_sids)\n",
        "\n",
        "acc  = accuracy_score(y_true_cls, y_pred_cls)\n",
        "bacc = balanced_accuracy_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "_, _, f_macro,  _ = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=\"macro\",   zero_division=0)\n",
        "_, _, f_weight, _ = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=\"weighted\", zero_division=0)\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=None, zero_division=0)\n",
        "\n",
        "cm  = confusion_matrix(y_true_cls, y_pred_cls, labels=LABELS)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "    index=[f\"True_{l}\" for l in LABELS],\n",
        "    columns=[f\"Pred_{l}\" for l in LABELS])\n",
        "\n",
        "ei  = LABELS.index(\"Extreme\")\n",
        "te  = cm[ei].sum()\n",
        "me  = cm[ei, LABELS.index(\"Low\")] + cm[ei, LABELS.index(\"Medium\")]\n",
        "sdr = 1 - (me / te) if te > 0 else 0\n",
        "cat = sum(cm[i,j] for i in range(4) for j in range(4) if abs(i-j)>=2) / cm.sum()\n",
        "kap = cohen_kappa_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "per_class = pd.DataFrame({\n",
        "    \"Level\":     LABELS,\n",
        "    \"Precision\": np.round(p_cls, 2),\n",
        "    \"Recall\":    np.round(r_cls, 2),\n",
        "    \"F1\":        np.round(f_cls, 2),\n",
        "    \"Support\":   support\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"  SARIMA (Statistical Baseline) — Rolling Forecast\")\n",
        "print(\"=\"*55)\n",
        "print(f\"  RMSE              : {rmse:,.2f}\")\n",
        "print(f\"  MAE               : {mae:,.2f}\")\n",
        "print(f\"  R²                : {r2:.4f}\")\n",
        "print(f\"  Accuracy          : {round(acc,4)}\")\n",
        "print(f\"  Balanced Accuracy : {round(bacc,4)}\")\n",
        "print(f\"  Macro F1          : {round(f_macro,4)}\")\n",
        "print(f\"  Weighted F1       : {round(f_weight,4)}\")\n",
        "print(f\"  Severe Detection  : {round(sdr,4)}\")\n",
        "print(f\"  Cat Error         : {round(cat,4)}\")\n",
        "print(f\"  Cohen's Kappa     : {round(kap,4)}\")\n",
        "print(f\"\\n  Per-class:\\n{per_class.to_string(index=False)}\")\n",
        "print(f\"\\n  Confusion Matrix:\\n{cm_df}\")\n",
        "\n",
        "sarima_results = {\n",
        "    \"rmse\":rmse, \"mae\":mae, \"r2\":r2,\n",
        "    \"acc\":acc, \"bacc\":bacc,\n",
        "    \"macro_f1\":f_macro, \"weighted_f1\":f_weight,\n",
        "    \"severe_det\":sdr, \"cat_rate\":cat, \"kappa\":kap\n",
        "}\n",
        "print(\"\\nsarima_results ✅\")\n"
      ],
      "metadata": {
        "id": "Yi1qAdBm2LiG",
        "outputId": "f26610b2-52e2-45f4-964b-fb04c6719cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data ready: (194400, 27)\n",
            "\n",
            "Fitting ARIMA with rolling forecast...\n",
            "  S1... ✅ (162 points)\n",
            "  S2... ✅ (162 points)\n",
            "  S3... ✅ (162 points)\n",
            "  S4... ✅ (162 points)\n",
            "  S5... ✅ (162 points)\n",
            "  S6... ✅ (162 points)\n",
            "\n",
            "=======================================================\n",
            "  SARIMA (Statistical Baseline) — Rolling Forecast\n",
            "=======================================================\n",
            "  RMSE              : 1,004.51\n",
            "  MAE               : 722.32\n",
            "  R²                : 0.4068\n",
            "  Accuracy          : 0.5412\n",
            "  Balanced Accuracy : 0.4303\n",
            "  Macro F1          : 0.4358\n",
            "  Weighted F1       : 0.5522\n",
            "  Severe Detection  : 0.4828\n",
            "  Cat Error         : 0.1348\n",
            "  Cohen's Kappa     : 0.3233\n",
            "\n",
            "  Per-class:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.84    0.75 0.79      470\n",
            " Medium       0.33    0.44 0.38      240\n",
            "   High       0.23    0.24 0.24      146\n",
            "Extreme       0.39    0.29 0.33      116\n",
            "\n",
            "  Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low           351           88         28             3\n",
            "True_Medium         44          106         64            26\n",
            "True_High           14           73         35            24\n",
            "True_Extreme         7           53         22            34\n",
            "\n",
            "sarima_results ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Masar — XGBoost 30-Minute Crowd Forecast\n",
        "نفس الـ split الجديد (70/15/15) مع نفس df\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix,\n",
        "    cohen_kappa_score\n",
        ")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 1) FEATURE COLUMNS\n",
        "# ─────────────────────────────────────────\n",
        "FEATURES = [\n",
        "    \"hour\", \"minute_of_day\", \"day_of_week\", \"is_weekend\",\n",
        "    \"station_id\", \"headway_seconds\",\n",
        "    \"event_flag\", \"holiday_flag\", \"special_event_type\",\n",
        "    \"lag_5\", \"lag_15\", \"lag_30\", \"lag_60\", \"lag_120\",\n",
        "    \"roll_mean_15\", \"roll_std_15\", \"roll_mean_60\",\n",
        "]\n",
        "TARGET = \"target_30m\"\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 2) نفس الـ SPLIT بالضبط\n",
        "# ─────────────────────────────────────────\n",
        "df_sorted = df.sort_values([\"timestamp\", \"station_id\"]).reset_index(drop=True)\n",
        "\n",
        "X_sorted = df_sorted[FEATURES].copy()\n",
        "y_sorted = df_sorted[TARGET].copy()\n",
        "\n",
        "n = len(df_sorted)\n",
        "train_end = int(n * 0.70)\n",
        "val_end   = int(n * 0.85)\n",
        "\n",
        "X_train = X_sorted.iloc[:train_end]\n",
        "y_train = y_sorted.iloc[:train_end]\n",
        "\n",
        "X_val   = X_sorted.iloc[train_end:val_end]\n",
        "y_val   = y_sorted.iloc[train_end:val_end]\n",
        "\n",
        "X_test  = X_sorted.iloc[val_end:]\n",
        "y_test  = y_sorted.iloc[val_end:]\n",
        "\n",
        "print(f\"Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 3) TRAIN XGBOOST\n",
        "# ─────────────────────────────────────────\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.07,\n",
        "    max_depth=5,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 4) REGRESSION METRICS\n",
        "# ─────────────────────────────────────────\n",
        "y_pred_reg = xgb_model.predict(X_test)\n",
        "y_true_reg = y_test.values\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
        "mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "r2   = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "print(\"\\n========== REGRESSION METRICS (XGBoost) ==========\")\n",
        "print(f\"RMSE : {rmse:,.2f}\")\n",
        "print(f\"MAE  : {mae:,.2f}\")\n",
        "print(f\"R²   : {r2:.4f}\")\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 5) تحويل → congestion levels\n",
        "# ─────────────────────────────────────────\n",
        "CAP    = 4800\n",
        "LABELS = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "def to_level(x, cap=CAP):\n",
        "    r = x / cap\n",
        "    if r < 0.40:   return \"Low\"\n",
        "    elif r < 0.70: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "y_true_cls = [to_level(v) for v in y_true_reg]\n",
        "y_pred_cls = [to_level(v) for v in y_pred_reg]\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 6) CLASSIFICATION METRICS\n",
        "# ─────────────────────────────────────────\n",
        "acc  = accuracy_score(y_true_cls, y_pred_cls)\n",
        "bacc = balanced_accuracy_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=\"macro\", zero_division=0)\n",
        "p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=\"weighted\", zero_division=0)\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_true_cls, y_pred_cls, labels=LABELS, average=None, zero_division=0)\n",
        "\n",
        "per_class = pd.DataFrame({\n",
        "    \"Level\":     LABELS,\n",
        "    \"Precision\": np.round(p_cls, 2),\n",
        "    \"Recall\":    np.round(r_cls, 2),\n",
        "    \"F1\":        np.round(f_cls, 2),\n",
        "    \"Support\":   support\n",
        "})\n",
        "\n",
        "cm = confusion_matrix(y_true_cls, y_pred_cls, labels=LABELS)\n",
        "cm_df = pd.DataFrame(cm,\n",
        "    index=[f\"True_{l}\" for l in LABELS],\n",
        "    columns=[f\"Pred_{l}\" for l in LABELS])\n",
        "\n",
        "print(\"\\n========== CLASSIFICATION METRICS (XGBoost) ==========\")\n",
        "print(f\"Accuracy         : {round(acc, 4)}\")\n",
        "print(f\"Balanced Accuracy: {round(bacc, 4)}\")\n",
        "print(f\"\\nMacro  P/R/F1 : {round(p_macro,4)} {round(r_macro,4)} {round(f_macro,4)}\")\n",
        "print(f\"Weighted P/R/F1: {round(p_weight,4)} {round(r_weight,4)} {round(f_weight,4)}\")\n",
        "print(\"\\nPer-class table:\")\n",
        "print(per_class.to_string(index=False))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "# ─────────────────────────────────────────\n",
        "# 7) CUSTOM OPERATIONAL METRICS\n",
        "# ─────────────────────────────────────────\n",
        "extreme_idx  = LABELS.index(\"Extreme\")\n",
        "true_extreme = cm[extreme_idx].sum()\n",
        "missed_extreme = (cm[extreme_idx, LABELS.index(\"Low\")] +\n",
        "                  cm[extreme_idx, LABELS.index(\"Medium\")])\n",
        "severe_detection_rate = 1 - (missed_extreme / true_extreme)\n",
        "\n",
        "catastrophic = sum(\n",
        "    cm[i, j]\n",
        "    for i in range(len(LABELS))\n",
        "    for j in range(len(LABELS))\n",
        "    if abs(i - j) >= 2\n",
        ")\n",
        "catastrophic_rate = catastrophic / cm.sum()\n",
        "kappa = cohen_kappa_score(y_true_cls, y_pred_cls)\n",
        "\n",
        "print(\"\\n========== CUSTOM METRICS (XGBoost) ==========\")\n",
        "print(f\"Severe Congestion Detection Rate : {round(severe_detection_rate, 4)}\")\n",
        "print(f\"Catastrophic Error Rate          : {round(catastrophic_rate, 4)}\")\n",
        "print(f\"Cohen's Kappa                    : {round(kappa, 4)}\")"
      ],
      "metadata": {
        "id": "1YlAV_VjsYy-",
        "outputId": "079b23fa-1150-443c-f9f0-55b4ceb1d384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (136080, 17) | Val: (29160, 17) | Test: (29160, 17)\n",
            "Training finished.\n",
            "\n",
            "========== REGRESSION METRICS (XGBoost) ==========\n",
            "RMSE : 376.59\n",
            "MAE  : 202.33\n",
            "R²   : 0.9172\n",
            "\n",
            "========== CLASSIFICATION METRICS (XGBoost) ==========\n",
            "Accuracy         : 0.8951\n",
            "Balanced Accuracy: 0.6974\n",
            "\n",
            "Macro  P/R/F1 : 0.724 0.6974 0.7082\n",
            "Weighted P/R/F1: 0.8897 0.8951 0.8917\n",
            "\n",
            "Per-class table:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.97    0.97 0.97    19825\n",
            " Medium       0.81    0.85 0.83     6272\n",
            "   High       0.55    0.42 0.47     1949\n",
            "Extreme       0.57    0.54 0.56     1114\n",
            "\n",
            "Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         19323          502          0             0\n",
            "True_Medium        656         5360        248             8\n",
            "True_High            1          692        815           441\n",
            "True_Extreme         1           84        425           604\n",
            "\n",
            "========== CUSTOM METRICS (XGBoost) ==========\n",
            "Severe Congestion Detection Rate : 0.9237\n",
            "Catastrophic Error Rate          : 0.0032\n",
            "Cohen's Kappa                    : 0.7817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,        # نرفع شوي عشان ما ينهار\n",
        "    max_depth=6,            # بدل 4\n",
        "    min_samples_split=15,   # أقل تشدد من 25\n",
        "    min_samples_leaf=8,     # أقل تشدد من 15\n",
        "    max_features=0.4,       # يشوف ميزات أكثر شوي\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDt9QHddMDcD",
        "outputId": "ed8d5c41-44c8-43d7-ce5d-3d329b73cbe4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1) prediction from Random Forest\n",
        "# ===============================\n",
        "\n",
        "y_pred_reg = rf_model.predict(X_test)   # التوقع العددي\n",
        "y_true_reg = y_test.values              # القيم الحقيقية\n",
        "\n",
        "# ===============================\n",
        "# 2) تحويل إلى مستويات ازدحام\n",
        "# ===============================\n",
        "\n",
        "def level(x, cap=4800):  # سعة محطة KAFD\n",
        "    r = x / cap\n",
        "\n",
        "    if r < 0.40:\n",
        "        return \"Low\"\n",
        "    elif r < 0.70:\n",
        "        return \"Medium\"\n",
        "    elif r < 0.90:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Extreme\"\n",
        "\n",
        "y_true = [level(v) for v in y_true_reg]\n",
        "y_pred = [level(v) for v in y_pred_reg]\n",
        "\n",
        "print(\"Sample predictions:\")\n",
        "for i in range(5):\n",
        "    print(y_true[i], \"→\", y_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCoMJkcnMgBO",
        "outputId": "1c90e074-9149-4b16-c629-411a6e267f6a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions:\n",
            "Extreme → Medium\n",
            "Medium → Medium\n",
            "Medium → Medium\n",
            "Medium → Medium\n",
            "Low → Low\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "labels = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "# 1) Accuracy + Balanced Accuracy\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "bacc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "# 2) Macro / Weighted Precision, Recall, F1\n",
        "p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        ")\n",
        "p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, average=\"weighted\", zero_division=0\n",
        ")\n",
        "\n",
        "# 3) Per-class metrics\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        ")\n",
        "\n",
        "table = pd.DataFrame({\n",
        "    \"Level\": labels,\n",
        "    \"Precision\": np.round(p_cls, 2),\n",
        "    \"Recall\": np.round(r_cls, 2),\n",
        "    \"F1\": np.round(f_cls, 2),\n",
        "    \"Support\": support\n",
        "})\n",
        "\n",
        "# 4) Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[f\"True_{l}\" for l in labels],\n",
        "    columns=[f\"Pred_{l}\" for l in labels]\n",
        ")\n",
        "\n",
        "print(\"RF Accuracy:\", round(acc, 4))\n",
        "print(\"RF Balanced Accuracy:\", round(bacc, 4))\n",
        "print(\"\\nRF Macro  P/R/F1:\", round(p_macro, 4), round(r_macro, 4), round(f_macro, 4))\n",
        "print(\"RF Weighted P/R/F1:\", round(p_weight, 4), round(r_weight, 4), round(f_weight, 4))\n",
        "\n",
        "print(\"\\nPer-class table:\")\n",
        "print(table.to_string(index=False))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgUQmUwuMuoB",
        "outputId": "516fefd9-03f5-4dfa-a9e6-27ae603eb4d0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Accuracy: 0.8136\n",
            "RF Balanced Accuracy: 0.5421\n",
            "\n",
            "RF Macro  P/R/F1: 0.7006 0.5421 0.5336\n",
            "RF Weighted P/R/F1: 0.8437 0.8136 0.8063\n",
            "\n",
            "Per-class table:\n",
            "  Level  Precision  Recall   F1  Support\n",
            "    Low       0.97    0.88 0.92    19825\n",
            " Medium       0.57    0.90 0.70     6272\n",
            "   High       0.58    0.35 0.43     1949\n",
            "Extreme       0.69    0.05 0.09     1114\n",
            "\n",
            "Confusion Matrix:\n",
            "              Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         17353         2472          0             0\n",
            "True_Medium        609         5644         19             0\n",
            "True_High            3         1246        677            23\n",
            "True_Extreme         2          590        471            51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true_num, y_pred_num))\n",
        "mae = mean_absolute_error(y_true_num, y_pred_num)\n",
        "r2 = r2_score(y_true_num, y_pred_num)\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R2:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQmZERDFotOj",
        "outputId": "f4e3b1f8-1a94-4d5d-b1fc-a9c2b1f50e35"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.14272630815578646\n",
            "MAE: 0.06135288065843622\n",
            "R2: 0.6017663159909699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "labels = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "# تأكد y_train / y_test نصوص من نفس الفئات\n",
        "y_train = pd.Series(y_train).astype(str)\n",
        "y_test  = pd.Series(y_test).astype(str)\n",
        "\n",
        "# خففي الذاكرة\n",
        "X_train_np = X_train.to_numpy(dtype=np.float32, copy=False)\n",
        "X_test_np  = X_test.to_numpy(dtype=np.float32, copy=False)\n",
        "\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=120,        # أقل\n",
        "    max_depth=16,            # محدود\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    max_features=\"sqrt\",\n",
        "    class_weight=None,       # شيلينه عشان السرعة (بنوازن بطريقة ثانية لو نحتاج)\n",
        "    bootstrap=True,\n",
        "    max_samples=0.25,        # 🔥 كل شجرة تشوف 25% فقط من الداتا\n",
        "    n_jobs=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "rf_clf.fit(X_train_np, y_train)\n",
        "print(\"RF fit done in (sec):\", round(time.time() - t0, 2))\n",
        "\n",
        "rf_pred = rf_clf.predict(X_test_np)\n",
        "print(\"Pred done.\")"
      ],
      "metadata": {
        "id": "98hVijAb4wRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=180,      # أقل بكثير\n",
        "    max_depth=16,          # أهم تعديل (يمنع انفجار الذاكرة)\n",
        "    min_samples_split=12,\n",
        "    min_samples_leaf=5,\n",
        "    max_features=0.7,      # أسرع من sqrt\n",
        "    bootstrap=True,\n",
        "    n_jobs=2,              # لا تستخدم -1\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "pred_rf = rf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "1XoOiJ8okONe",
        "outputId": "a1119765-5d88-41e3-fc6a-2583285fe434"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3602257142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpred_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 3) Metrics\n",
        "# ==============================\n",
        "acc  = accuracy_score(y_test, rf_pred)\n",
        "bacc = balanced_accuracy_score(y_test, rf_pred)\n",
        "\n",
        "p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "    y_test, rf_pred, labels=labels, average=\"macro\", zero_division=0\n",
        ")\n",
        "p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "    y_test, rf_pred, labels=labels, average=\"weighted\", zero_division=0\n",
        ")\n",
        "\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_test, rf_pred, labels=labels, average=None, zero_division=0\n",
        ")\n",
        "\n",
        "table = pd.DataFrame({\n",
        "    \"Level\": labels,\n",
        "    \"Precision\": np.round(p_cls, 2),\n",
        "    \"Recall\": np.round(r_cls, 2),\n",
        "    \"F1\": np.round(f_cls, 2),\n",
        "    \"Support\": support\n",
        "})\n",
        "\n",
        "cm = confusion_matrix(y_test, rf_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[f\"True_{l}\" for l in labels],\n",
        "    columns=[f\"Pred_{l}\" for l in labels]\n",
        ")\n",
        "\n",
        "print(\"\\n================ RF (CLASSIFICATION) ================\")\n",
        "print(\"RF Accuracy:\", round(acc, 4))\n",
        "print(\"RF Balanced Accuracy:\", round(bacc, 4))\n",
        "print(\"\\nRF Macro  P/R/F1:\", round(p_macro, 4), round(r_macro, 4), round(f_macro, 4))\n",
        "print(\"RF Weighted P/R/F1:\", round(p_weight, 4), round(r_weight, 4), round(f_weight, 4))\n",
        "\n",
        "print(\"\\nPer-class table:\")\n",
        "print(table.to_string(index=False))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "s3eQidRwxhC2",
        "outputId": "37b42cdb-0e40-4809-ee22-ea2476c87aef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_pred' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4037640815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 3) Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ==============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0macc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalanced_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, confusion_matrix\n",
        ")\n",
        "\n",
        "# ترتيب الفئات (ثابتيه بالورقة)\n",
        "labels = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "# ==============================\n",
        "# 1) Train RF Classifier (SAFE)\n",
        "# ==============================\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=150,       # أقل من 400 لتجنب الكراش\n",
        "    max_depth=18,           # مهم جداً: يمنع الأشجار تصير عميقة بشكل مبالغ\n",
        "    min_samples_split=12,\n",
        "    min_samples_leaf=6,\n",
        "    max_features=0.7,       # أسرع من sqrt غالبًا\n",
        "    bootstrap=True,\n",
        "    n_jobs=1,               # لا تحطين -1 عشان ما ينهار الرن\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "cp8LOAQKjxet"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "2ALT-UdVxAFA",
        "outputId": "fbc3beae-3a92-4080-cb9e-d9792af9a1af"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-681854635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 2) Predict\n",
        "# ==============================\n",
        "rf_pred = rf_clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "qpydEZBexEkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "def evaluate_classifier(y_true, y_pred, model_name=\"Model\"):\n",
        "\n",
        "    labels = [\"Low\", \"Medium\", \"High\", \"Extreme\"]\n",
        "\n",
        "    # 1) Accuracy + Balanced Accuracy\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # 2) Macro / Weighted Precision, Recall, F1\n",
        "    p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "\n",
        "    # 3) Per-class metrics\n",
        "    p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    table = pd.DataFrame({\n",
        "        \"Level\": labels,\n",
        "        \"Precision\": np.round(p_cls, 2),\n",
        "        \"Recall\": np.round(r_cls, 2),\n",
        "        \"F1\": np.round(f_cls, 2),\n",
        "        \"Support\": support\n",
        "    })\n",
        "\n",
        "    # 4) Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[f\"True_{l}\" for l in labels],\n",
        "                         columns=[f\"Pred_{l}\" for l in labels])\n",
        "\n",
        "    print(f\"\\n================ {model_name} (CLASSIFICATION) ================\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"Balanced Accuracy:\", round(bacc, 4))\n",
        "\n",
        "    print(\"\\nMacro  P/R/F1:\", round(p_macro, 4), round(r_macro, 4), round(f_macro, 4))\n",
        "    print(\"Weighted P/R/F1:\", round(p_weight, 4), round(r_weight, 4), round(f_weight, 4))\n",
        "\n",
        "    print(\"\\nPer-class table:\")\n",
        "    print(table.to_string(index=False))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm_df)\n",
        "\n",
        "    return acc, bacc, f_macro, f_weight"
      ],
      "metadata": {
        "id": "RKclIjrOEXwE"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_pred = rf_clf.predict(X_test)\n",
        "\n",
        "evaluate_classifier(y_test, rf_pred, \"Random Forest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Gg6USWfYEkFA",
        "outputId": "ce51357f-0cca-44ae-f674-f729e8d3e950"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_clf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3846831086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Random Forest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selected Configuration\n",
        "\n",
        "We select **`shallower_faster`** as the **final model configuration** because:\n",
        "\n",
        "- It achieves the **lowest RMSE** and **highest R²** on the validation set.  \n",
        "- The model is **simpler (shallower trees)** and therefore more stable and efficient  \n",
        "  for deployment in Masar’s real-time Digital Twin.\n",
        "\n"
      ],
      "metadata": {
        "id": "oOUCzzpyPg0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Evaluation on Test Set\n"
      ],
      "metadata": {
        "id": "bBd5PcyhPi-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate selected model on the Test set\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "mae_test  = mean_absolute_error(y_test, y_test_pred)\n",
        "r2_test   = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Final Test Metrics:\")\n",
        "print(f\"  RMSE: {rmse_test:,.2f}\")\n",
        "print(f\"  MAE : {mae_test:,.2f}\")\n",
        "print(f\"  R²  : {r2_test:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p6mKAOjRw-c",
        "outputId": "890b524b-205f-4d7c-9a28-a2039340bb60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Metrics:\n",
            "  RMSE: 311.50\n",
            "  MAE : 162.70\n",
            "  R²  : 0.906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:**  \n",
        "> The Test set shows exceptionally high performance because its time window is\n",
        "> very similar to the Training data and contains **fewer special events or\n",
        "> irregular spikes**.  \n",
        ">\n",
        "> As a result, the model performs extremely well on Test data since the patterns\n",
        "> are consistent (regular weekdays, stable demand, limited event variability).  \n",
        ">\n",
        "> In real-world deployment, performance may vary slightly during weeks with\n",
        "> higher event activity or unusual crowd surges.\n"
      ],
      "metadata": {
        "id": "ZrOVFf-bSjAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Visualizing Predictions: Validation Set & Test Set\n"
      ],
      "metadata": {
        "id": "rleHKWoUThQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Final XGBoost Model for Deployment\n"
      ],
      "metadata": {
        "id": "LOHysQbuSkre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "MODEL_PATH = \"masar_xgb_30min_model.pkl\"\n",
        "joblib.dump(best_model, MODEL_PATH)\n",
        "\n",
        "print(f\"Model saved successfully → {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpHdEnDwSzk1",
        "outputId": "3bd8cb53-9c2b-4344-91d2-a00579a480e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully → masar_xgb_30min_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 1) اختاري الأعمدة\n",
        "feature_cols = [\n",
        "    \"lag_5\",\"lag_15\",\"lag_30\",\"lag_60\",\"lag_120\",\n",
        "    \"roll_mean_15\",\"roll_std_15\",\"roll_mean_60\",\n",
        "    \"headway_seconds\",\"is_weekend\",\"event_flag\",\"holiday_flag\"\n",
        "]\n",
        "target_col = \"target_30min\"\n",
        "\n",
        "# تأكد إن df_model موجود\n",
        "assert \"df_model\" in globals(), \"df_model غير معرف — شغلي خطوة feature engineering + dropna أول\"\n",
        "assert set(feature_cols + [target_col]).issubset(df_model.columns)\n",
        "\n",
        "# 2) Split زمني (مهم للـ time series) — آخر 20% Test\n",
        "df_model = df_model.sort_values([\"station_id\",\"timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "cut = int(len(df_model) * 0.8)\n",
        "train_df = df_model.iloc[:cut].copy()\n",
        "test_df  = df_model.iloc[cut:].copy()\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[target_col].astype(float)\n",
        "\n",
        "X_test  = test_df[feature_cols]\n",
        "y_test  = test_df[target_col].astype(float)\n",
        "\n",
        "# 3) Model\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=8,\n",
        "    min_samples_leaf=3,\n",
        "    max_features=\"sqrt\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 4) Train\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# 5) Predict + Metrics\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "mae  = mean_absolute_error(y_test, y_pred)\n",
        "r2   = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Random Forest (30-min ahead) — TEST\")\n",
        "print(\"RMSE:\", round(rmse, 2))\n",
        "print(\"MAE :\", round(mae, 2))\n",
        "print(\"R^2 :\", round(r2, 3))\n",
        "\n",
        "# 6) (اختياري) حفظ التوقعات للمقارنة\n",
        "pred_out = test_df[[\"timestamp\",\"station_id\"]].copy()\n",
        "pred_out[\"y_true\"] = y_test.values\n",
        "pred_out[\"y_pred_rf\"] = y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJdl1V80JkL3",
        "outputId": "82d3e723-b2c4-4080-f26a-d6e0603f04df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (30-min ahead) — TEST\n",
            "RMSE: 441.39\n",
            "MAE : 243.21\n",
            "R^2 : 0.425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# بعد ما تسوين split (train_df / test_df)\n",
        "# خلينا نبني y_true/y_pred من نفس test_df\n",
        "\n",
        "# القيم الحقيقية (regression target)\n",
        "y_true_reg = test_df[\"target_30min\"].values\n",
        "\n",
        "# توقعات RF (regression)\n",
        "y_pred_reg = rf.predict(test_df[feature_cols])"
      ],
      "metadata": {
        "id": "KhiiPRwXN-Q_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"Low\",\"Medium\",\"High\",\"Extreme\"]\n",
        "\n",
        "def level(x, cap=4800):\n",
        "    r = x / cap\n",
        "    if r < 0.35: return \"Low\"\n",
        "    elif r < 0.65: return \"Medium\"\n",
        "    elif r < 0.90: return \"High\"\n",
        "    else: return \"Extreme\"\n",
        "\n",
        "y_true = [level(v) for v in y_true_reg]\n",
        "y_pred = [level(v) for v in y_pred_reg]\n",
        "\n",
        "print(len(y_true), len(y_pred))  # لازم يطلع نفس الرقم"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al_Z-ey1OAGY",
        "outputId": "81da54bb-7c3b-4f6c-a7c5-59387e5d9995"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38736 38736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "labels = [\"Low\",\"Medium\",\"High\",\"Extreme\"]\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred)\n",
        "bacc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "p_cls, r_cls, f_cls, support = precision_recall_fscore_support(\n",
        "    y_true, y_pred, labels=labels, zero_division=0\n",
        ")\n",
        "\n",
        "p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "p_weight, r_weight, f_weight, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "table_rf = pd.DataFrame({\n",
        "    \"Level\": labels,\n",
        "    \"RF Precision\": np.round(p_cls,2),\n",
        "    \"RF Recall\": np.round(r_cls,2),\n",
        "    \"RF F1\": np.round(f_cls,2),\n",
        "    \"Support\": support\n",
        "})\n",
        "\n",
        "print(\"RF Accuracy:\", round(acc,4))\n",
        "print(\"RF Balanced Accuracy:\", round(bacc,4))\n",
        "print(\"RF Macro P/R/F1:\", round(p_macro,4), round(r_macro,4), round(f_macro,4))\n",
        "print(\"RF Weighted P/R/F1:\", round(p_weight,4), round(r_weight,4), round(f_weight,4))\n",
        "print(\"\\nPer-class table:\\n\", table_rf)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"True_{l}\" for l in labels], columns=[f\"Pred_{l}\" for l in labels])\n",
        "print(\"\\nConfusion Matrix:\\n\", cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftKQ9pMTODhG",
        "outputId": "b6e48ce5-eeb9-4e67-ce88-eb22f9817a2f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Accuracy: 0.8136\n",
            "RF Balanced Accuracy: 0.5421\n",
            "RF Macro P/R/F1: 0.7006 0.5421 0.5336\n",
            "RF Weighted P/R/F1: 0.8437 0.8136 0.8063\n",
            "\n",
            "Per-class table:\n",
            "      Level  RF Precision  RF Recall  RF F1  Support\n",
            "0      Low          0.97       0.88   0.92    19825\n",
            "1   Medium          0.57       0.90   0.70     6272\n",
            "2     High          0.58       0.35   0.43     1949\n",
            "3  Extreme          0.69       0.05   0.09     1114\n",
            "\n",
            "Confusion Matrix:\n",
            "               Pred_Low  Pred_Medium  Pred_High  Pred_Extreme\n",
            "True_Low         17353         2472          0             0\n",
            "True_Medium        609         5644         19             0\n",
            "True_High            3         1246        677            23\n",
            "True_Extreme         2          590        471            51\n"
          ]
        }
      ]
    }
  ]
}