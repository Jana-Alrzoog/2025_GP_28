{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jana-Alrzoog/2025_GP_28/blob/main/masar-sim/notebooks/masar_occupancy_week.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸšŒ **Masar Occupancy â€” Week Generator**\n",
        "\n",
        "This notebook generates **minute-level passenger occupancy** for a **7-day window** across selected stations/lines.  \n",
        "It starts from the **base curve** and applies **context modifiers** (station capacity, weekend, weather, events; holidays per config),  \n",
        "then exports tidy CSVs for dashboards and Firestore/pipeline publishing.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ **Purpose**\n",
        "- Produce **7 consecutive daily time series** at **1-minute resolution**.\n",
        "- Fill any **missing days** in the target week with a consistent **template day**.\n",
        "- Output clean, validated CSVs for QA, visualization, and aggregation.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§© **Inputs**\n",
        "| File / Seed | Description |\n",
        "|---|---|\n",
        "| `base_week.csv` or `base_day.csv Ã— 7` | Base minute-level demand (from **masar_base_demand.ipynb**) |\n",
        "| Seeds | `stations`, `events`, `weather` *(holidays controlled via config)* |\n",
        "| Config | `00_config.yaml` *(paths, multipliers, timezone, resolution, headways)* |\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ **Workflow**\n",
        "1ï¸âƒ£ **Load config & seeds** (paths, stations, events, weather).  \n",
        "2ï¸âƒ£ **Select week window** (e.g., `2025-09-21 â†’ 2025-09-27`) and ensure TZ & minute resolution.  \n",
        "3ï¸âƒ£ **Build minute grid per day & station**; **expand missing days** using the densest template day.  \n",
        "4ï¸âƒ£ **Compute modifiers** per minute:\n",
        "   - `station_scale` (capacity vs. network mean)  \n",
        "   - `weekend_mult` (Fri/Sat)  \n",
        "   - `weather_mult` (Sunny/Dusty/Rainyâ€¦)  \n",
        "   - `event_mult` (supports global and station-scoped events)  \n",
        "   - `holiday_mult` *(enabled/disabled via config)*\n",
        "5ï¸âƒ£ **Finalize demand** â†’ normalize per station, map to `station_total`, derive `crowd_level`.  \n",
        "6ï¸âƒ£ **Headways** from config peak/off-peak patterns.  \n",
        "7ï¸âƒ£ **QA checks** (non-negative totals, station coverage, event flags).  \n",
        "8ï¸âƒ£ **Export** per-day CSVs **and** a consolidated weekly CSV.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "p5B-pJq3CY3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Jana-Alrzoog/2025_GP_28.git\n",
        "%cd /content/2025_GP_28/masar-sim\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwrEB5u32MD6",
        "outputId": "c2127fc2-d038-4388-de41-d4fb2b063285"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path '2025_GP_28' already exists and is not an empty directory.\n",
            "/content/2025_GP_28/masar-sim\n",
            "data  lib  notebooks  sims\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB-p9pLs101k",
        "outputId": "e5339477-445d-4006-d4a2-43d9e5a09202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT = /content/2025_GP_28/masar-sim\n",
            "GEN  = /content/2025_GP_28/masar-sim/data/generated\n",
            "CONF = /content/2025_GP_28/masar-sim/sims/00_config.yaml\n",
            "base_day rows=6,486, stations=6, day=2025-09-24\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# masar_occupancy_week.ipynb\n",
        "# Generate a full week with changing scenarios â†’ occupancy_week.csv\n",
        "# =========================================================\n",
        "\n",
        "import os, json, csv, yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil.parser import parse\n",
        "\n",
        "CANDIDATES = [\n",
        "    \"/content/2025_GP_28_latest/masar-sim\",\n",
        "    \"/content/2025_GP_28/masar-sim\",\n",
        "    \"/content/masar-sim\",\n",
        "]\n",
        "ROOT = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
        "assert ROOT, \"Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¬Ù„Ø¯ masar-sim. ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ø§Ù„ÙƒÙ„ÙˆÙ† ÙˆØ§Ù„Ù…Ø³Ø§Ø±.\"\n",
        "SEED = f\"{ROOT}/data/seeds\"\n",
        "GEN  = f\"{ROOT}/data/generated\"\n",
        "CONF = f\"{ROOT}/sims/00_config.yaml\"\n",
        "\n",
        "print(\"ROOT =\", ROOT)\n",
        "print(\"GEN  =\", GEN)\n",
        "print(\"CONF =\", CONF)\n",
        "\n",
        "with open(CONF) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "with open(f\"{SEED}/stations.json\") as f:\n",
        "    stations = json.load(f)\n",
        "with open(f\"{SEED}/weather_patterns.json\") as f:\n",
        "    weather_map = json.load(f)\n",
        "with open(f\"{SEED}/calendar_events.csv\") as f:\n",
        "    events_seed = list(csv.DictReader(f))\n",
        "\n",
        "\n",
        "base_path = f\"{GEN}/base_day.csv\"\n",
        "assert os.path.exists(base_path), \"base_day.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯â€”Ø´ØºÙ‘Ù„ÙŠ masar_base_demand.ipynb Ø£ÙˆÙ„Ù‹Ø§.\"\n",
        "base_day = pd.read_csv(base_path, parse_dates=[\"timestamp\"])\n",
        "print(f\"base_day rows={len(base_day):,}, stations={base_day['station_id'].nunique()}, day={base_day['timestamp'].dt.date.iloc[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(f\"{ROOT}/lib\")\n",
        "from modifiers import compute_demand_modifier\n"
      ],
      "metadata": {
        "id": "3dwvIH5Y2P_y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "week_start = parse(\"2025-09-21\")\n",
        "\n",
        "scenario_cycle = [\"normal\", \"rainy\", \"event_kafd\", \"normal\", \"holiday\", \"dusty\", \"normal\"]\n",
        "\n",
        "days = [\n",
        "    {\"date\": (week_start + timedelta(days=i)).date(), \"tag\": scenario_cycle[i % len(scenario_cycle)]}\n",
        "    for i in range(7)\n",
        "]\n",
        "days\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMk7n-E32VCz",
        "outputId": "3aa59237-bf68-4ef1-f229-112dd49e4117"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'date': datetime.date(2025, 9, 21), 'tag': 'normal'},\n",
              " {'date': datetime.date(2025, 9, 22), 'tag': 'rainy'},\n",
              " {'date': datetime.date(2025, 9, 23), 'tag': 'event_kafd'},\n",
              " {'date': datetime.date(2025, 9, 24), 'tag': 'normal'},\n",
              " {'date': datetime.date(2025, 9, 25), 'tag': 'holiday'},\n",
              " {'date': datetime.date(2025, 9, 26), 'tag': 'dusty'},\n",
              " {'date': datetime.date(2025, 9, 27), 'tag': 'normal'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.rename(\n",
        "    \"/content/2025_GP_28/masar-sim/data/generated/base_day.csv\",\n",
        "    \"/content/2025_GP_28/masar-sim/data/generated/day_base.csv\"\n",
        ")\n",
        "print(\"Renamed âœ“ base_day.csv â†’ day_base.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-mWpZi2Ey6V",
        "outputId": "00940cb9-4341-4346-a392-da9f51a39c22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed âœ“ base_day.csv â†’ day_base.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "req_cols = [\"station_id\", \"minute_of_day\", \"base_demand\"]\n",
        "# Auto-create minute_of_day if missing (0â€“1439)\n",
        "if \"minute_of_day\" not in base_day.columns:\n",
        "    if \"hour\" in base_day.columns and \"minute\" in base_day.columns:\n",
        "        base_day[\"minute_of_day\"] = base_day[\"hour\"] * 60 + base_day[\"minute\"]\n",
        "    elif \"timestamp\" in base_day.columns:\n",
        "        ts = pd.to_datetime(base_day[\"timestamp\"], errors=\"coerce\")\n",
        "        base_day[\"minute_of_day\"] = ts.dt.hour * 60 + ts.dt.minute\n",
        "    else:\n",
        "        raise KeyError(\"minute_of_day not found and no way to reconstruct it (need hour/minute or timestamp).\")\n"
      ],
      "metadata": {
        "id": "SmLkUuoyFPva"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Fix base_day columns and build minute_of_day if missing\n",
        "# Run this ONCE after loading `base_day` (or before bootstrap).\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "if 'base_day' not in globals():\n",
        "    # Try load if not in memory\n",
        "    import os\n",
        "    ROOT = \"/content/2025_GP_28/masar-sim\"\n",
        "    GEN  = f\"{ROOT}/data/generated\"\n",
        "    candidates = [\n",
        "        f\"{GEN}/day_base.csv\",\n",
        "        f\"{GEN}/base_day.csv\",           # your current file name\n",
        "        f\"{GEN}/day_demand_base.csv\",\n",
        "        f\"{ROOT}/data/base/day_base.csv\",\n",
        "        f\"{ROOT}/data/base/day_demand_base.csv\",\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            base_day = pd.read_csv(p)\n",
        "            print(\"Loaded base_day from:\", p)\n",
        "            break\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No base-day CSV found in common locations.\")\n",
        "\n",
        "# 1) normalize headers\n",
        "base_day.columns = [str(c).strip().lower() for c in base_day.columns]\n",
        "\n",
        "# 2) standardize common names\n",
        "rename_map = {\n",
        "    # station id\n",
        "    \"station\": \"station_id\",\n",
        "    \"station code\": \"station_id\",\n",
        "    \"station_code\": \"station_id\",\n",
        "    \"sid\": \"station_id\",\n",
        "    # base demand\n",
        "    \"base\": \"base_demand\",\n",
        "    \"basedemand\": \"base_demand\",\n",
        "    \"demand_base\": \"base_demand\",\n",
        "    \"base_day_demand\": \"base_demand\",\n",
        "    \"base_day\": \"base_demand\",\n",
        "    # minute of day\n",
        "    \"minute\": \"minute_of_day\",\n",
        "    \"min\": \"minute_of_day\",\n",
        "    \"minuteofday\": \"minute_of_day\",\n",
        "    \"minute-of-day\": \"minute_of_day\",\n",
        "}\n",
        "base_day = base_day.rename(columns=rename_map)\n",
        "\n",
        "# 3) build minute_of_day if missing\n",
        "if \"minute_of_day\" not in base_day.columns:\n",
        "    if {\"hour\",\"minute\"}.issubset(base_day.columns):\n",
        "        base_day[\"minute_of_day\"] = (\n",
        "            pd.to_numeric(base_day[\"hour\"], errors=\"coerce\").fillna(0).astype(int)*60 +\n",
        "            pd.to_numeric(base_day[\"minute\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "        )\n",
        "        print(\"Built minute_of_day from hour+minute âœ“\")\n",
        "    elif \"time\" in base_day.columns:\n",
        "        # supports \"HH:MM\" or \"HH:MM:SS\"\n",
        "        t = pd.to_datetime(base_day[\"time\"], errors=\"coerce\")\n",
        "        base_day[\"minute_of_day\"] = (t.dt.hour*60 + t.dt.minute).astype(\"Int64\").fillna(0).astype(int)\n",
        "        print(\"Built minute_of_day from time (HH:MM[:SS]) âœ“\")\n",
        "    elif \"timestamp\" in base_day.columns:\n",
        "        ts = pd.to_datetime(base_day[\"timestamp\"], errors=\"coerce\")\n",
        "        base_day[\"minute_of_day\"] = (ts.dt.hour*60 + ts.dt.minute).astype(\"Int64\").fillna(0).astype(int)\n",
        "        print(\"Built minute_of_day from timestamp âœ“\")\n",
        "    else:\n",
        "        # fallback: if rows are per-minute in order, use the index\n",
        "        if len(base_day) in (1440, 2880):  # minute-level (1 day / maybe 2)\n",
        "            base_day = base_day.reset_index().rename(columns={\"index\":\"minute_of_day\"})\n",
        "            base_day[\"minute_of_day\"] = base_day[\"minute_of_day\"].clip(0, 1439).astype(int)\n",
        "            print(\"Built minute_of_day from index fallback âœ“\")\n",
        "        else:\n",
        "            raise KeyError(\n",
        "                \"minute_of_day is missing and cannot be reconstructed.\\n\"\n",
        "                \"Provide either (hour, minute) OR a 'time' column OR 'timestamp'.\"\n",
        "            )\n",
        "\n",
        "# 4) ensure required columns and types\n",
        "if \"station_id\" not in base_day.columns:\n",
        "    # try recover from code/name columns if present\n",
        "    for cand in [\"code\",\"stationname\",\"name\"]:\n",
        "        if cand in base_day.columns:\n",
        "            base_day[\"station_id\"] = base_day[cand].astype(str)\n",
        "            print(f\"Created station_id from '{cand}' âœ“\")\n",
        "            break\n",
        "if \"station_id\" not in base_day.columns:\n",
        "    raise KeyError(\"Missing 'station_id' in base_day (and no alternative column found).\")\n",
        "\n",
        "base_day[\"station_id\"]    = base_day[\"station_id\"].astype(str).str.strip()\n",
        "base_day[\"minute_of_day\"] = pd.to_numeric(base_day[\"minute_of_day\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "# base_demand may be under a different name in your file; try a safe fallback\n",
        "if \"base_demand\" not in base_day.columns:\n",
        "    for cand in [\"demand\", \"baseline\", \"base\", \"y_base\"]:\n",
        "        if cand in base_day.columns:\n",
        "            base_day[\"base_demand\"] = pd.to_numeric(base_day[cand], errors=\"coerce\")\n",
        "            print(f\"Created base_demand from '{cand}' âœ“\")\n",
        "            break\n",
        "if \"base_demand\" not in base_day.columns:\n",
        "    raise KeyError(\"Missing 'base_demand' in base_day.\")\n",
        "\n",
        "base_day[\"base_demand\"] = pd.to_numeric(base_day[\"base_demand\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "print(\"Normalization âœ“  Columns:\", list(base_day.columns)[:10], \"â€¦\")\n",
        "print(base_day.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wyYYclUFtSK",
        "outputId": "dd5cf43d-27e7-4dba-e34b-9ecff4582b4b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built minute_of_day from timestamp âœ“\n",
            "Normalization âœ“  Columns: ['timestamp', 'station_id', 'base_demand', 'base_demand_norm', 'minute_of_day'] â€¦\n",
            "        timestamp station_id  base_demand  base_demand_norm  minute_of_day\n",
            "0  9/24/2025 6:00         S1     0.078566          0.074824            360\n",
            "1  9/24/2025 6:01         S1     0.080302          0.076478            361\n",
            "2  9/24/2025 6:02         S1     0.082128          0.078217            362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Bootstrap loader: build `base_df` for the week from `base_day` in memory\n",
        "#  - Uses the `base_day` dataframe already loaded and normalized.\n",
        "#  - Replicates the base-day minute grid across WEEK_START..WEEK_END\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# ---- Project roots (edit if your path differs)\n",
        "if 'ROOT' not in globals():\n",
        "    ROOT = \"/content/2025_GP_28/masar-sim\"\n",
        "SEED = f\"{ROOT}/data/seeds\"\n",
        "GEN  = f\"{ROOT}/data/generated\"\n",
        "\n",
        "# ---- Target week (keep in sync with your main script)\n",
        "WEEK_START = pd.Timestamp(\"2025-09-21\")\n",
        "WEEK_END   = pd.Timestamp(\"2025-09-27\")\n",
        "\n",
        "# Use the base_day DataFrame already loaded and normalized\n",
        "if 'base_day' not in globals():\n",
        "    raise RuntimeError(\"The 'base_day' DataFrame was not found in memory. Please run the preceding cells.\")\n",
        "\n",
        "# Ensure required columns exist after prior normalization\n",
        "req_cols = [\"station_id\", \"minute_of_day\", \"base_demand\"]\n",
        "missing = [c for c in req_cols if c not in base_day.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required column(s) in 'base_day' DataFrame: {missing}. Rerun cells that load and normalize base_day.\")\n",
        "\n",
        "# If timestamp exists we ignore its date and rebuild per target dates\n",
        "# Build a full week by repeating the base-day minute grid per date\n",
        "dates = pd.date_range(WEEK_START, WEEK_END, freq=\"D\")\n",
        "frames = []\n",
        "for d in dates:\n",
        "    df_d = base_day.copy()\n",
        "    h = (df_d[\"minute_of_day\"] // 60).astype(int)\n",
        "    m = (df_d[\"minute_of_day\"] %  60).astype(int)\n",
        "    df_d[\"date\"]      = d.strftime(\"%Y-%m-%d\")\n",
        "    df_d[\"timestamp\"] = pd.to_datetime(df_d[\"date\"] + \" \" + h.astype(str).str.zfill(2) + \":\" + m.astype(str).str.zfill(2) + \":00\")\n",
        "    df_d[\"hour\"]      = h\n",
        "    df_d[\"day_of_week\"] = d.weekday()\n",
        "    df_d[\"is_weekend\"]  = df_d[\"day_of_week\"].isin([4,5]).astype(int)  # Fri=4, Sat=5\n",
        "    frames.append(df_d)\n",
        "\n",
        "base_df = pd.concat(frames, ignore_index=True).sort_values(\n",
        "    [\"date\",\"station_id\",\"minute_of_day\"]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"base_df ready âœ“\")\n",
        "print(\"Dates:\", base_df[\"date\"].unique()[:3], \"...\", base_df[\"date\"].unique()[-3:])\n",
        "print(\"Rows:\", len(base_df), \"| Stations:\", base_df[\"station_id\"].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqednxgeEPTZ",
        "outputId": "84c007e5-b7f7-4d3e-89d2-bb194a9a956f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_df ready âœ“\n",
            "Dates: ['2025-09-21' '2025-09-22' '2025-09-23'] ... ['2025-09-25' '2025-09-26' '2025-09-27']\n",
            "Rows: 45402 | Stations: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, csv, json, yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ===================== 0) Data sources =====================\n",
        "# NOTE: If your pipeline used 'day_demand_base.csv', rename it to 'day_base.csv'.\n",
        "if 'week_df' in globals():\n",
        "    df = week_df.copy()\n",
        "elif 'base_df' in globals():\n",
        "    df = base_df.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"week_df or base_df not found in memory.\")\n",
        "\n",
        "# Project roots\n",
        "if 'ROOT' not in globals():\n",
        "    ROOT = \"/content/2025_GP_28/masar-sim\"\n",
        "SEED = f\"{ROOT}/data/seeds\"\n",
        "CONF = f\"{ROOT}/sims/00_config.yaml\"\n",
        "\n",
        "# Config\n",
        "with open(CONF, \"r\", encoding=\"utf-8\") as f:\n",
        "    config = yaml.safe_load(f) or {}\n",
        "\n",
        "# ===================== 1) Time fields + week window =====================\n",
        "ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "df[\"timestamp\"]     = ts\n",
        "df[\"date\"]          = ts.dt.strftime(\"%Y-%m-%d\")\n",
        "df[\"hour\"]          = ts.dt.hour\n",
        "df[\"minute_of_day\"] = df[\"hour\"]*60 + ts.dt.minute\n",
        "df[\"day_of_week\"]   = ts.dt.weekday\n",
        "if \"is_weekend\" not in df.columns:\n",
        "    # Asia/Riyadh: Fri=4, Sat=5\n",
        "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([4,5]).astype(int)\n",
        "\n",
        "# Target week: 21 â†’ 27 September 2025\n",
        "WEEK_START = pd.Timestamp(\"2025-09-21\")\n",
        "WEEK_END   = pd.Timestamp(\"2025-09-27\")\n",
        "mask_week  = (df[\"timestamp\"] >= WEEK_START) & (df[\"timestamp\"] <= WEEK_END + pd.Timedelta(days=1) - pd.Timedelta(seconds=1))\n",
        "df = df.loc[mask_week].copy()\n",
        "if df.empty:\n",
        "    raise RuntimeError(f\"No rows found within week window: {WEEK_START.date()} â†’ {WEEK_END.date()}\")\n",
        "\n",
        "# ========= 1.1) Fill missing days within week if needed =========\n",
        "def rebuild_ts(date_iso: str, minute_of_day: int) -> pd.Timestamp:\n",
        "    h = int(minute_of_day // 60)\n",
        "    m = int(minute_of_day % 60)\n",
        "    return pd.Timestamp(f\"{date_iso} {h:02d}:{m:02d}:00\")\n",
        "\n",
        "def expand_week_if_needed(df_week: pd.DataFrame,\n",
        "                          week_start: pd.Timestamp,\n",
        "                          week_end: pd.Timestamp) -> pd.DataFrame:\n",
        "    target_dates = [(week_start + pd.Timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "                    for i in range((week_end - week_start).days + 1)]\n",
        "    have_dates = sorted(df_week[\"date\"].unique().tolist())\n",
        "    missing = [d for d in target_dates if d not in have_dates]\n",
        "    if not missing:\n",
        "        print(\"No expansion needed. All week dates present âœ“\")\n",
        "        return df_week\n",
        "\n",
        "    # Use the densest day inside the window as a template\n",
        "    tmpl_date = df_week[\"date\"].value_counts().idxmax()\n",
        "    template  = df_week[df_week[\"date\"] == tmpl_date].copy()\n",
        "    keep_cols = template.columns.tolist()\n",
        "\n",
        "    clones = []\n",
        "    for d in missing:\n",
        "        c = template.copy()\n",
        "        # Update time fields\n",
        "        c[\"date\"] = d\n",
        "        c[\"timestamp\"] = c[\"minute_of_day\"].apply(lambda mo: rebuild_ts(d, int(mo)))\n",
        "        c[\"hour\"] = pd.to_datetime(c[\"timestamp\"]).dt.hour\n",
        "        c[\"day_of_week\"] = pd.to_datetime(c[\"timestamp\"]).dt.weekday\n",
        "        c[\"is_weekend\"]  = c[\"day_of_week\"].isin([4,5]).astype(int)\n",
        "        clones.append(c[keep_cols])\n",
        "\n",
        "    if clones:\n",
        "        df_week = pd.concat([df_week] + clones, axis=0, ignore_index=True)\n",
        "\n",
        "    print(f\"Expanded missing dates â†’ added {len(missing)} day(s): {missing}\")\n",
        "    # Sort after merge\n",
        "    df_week = df_week.sort_values([\"date\",\"station_id\",\"minute_of_day\"]).reset_index(drop=True)\n",
        "    return df_week\n",
        "\n",
        "df = expand_week_if_needed(df, WEEK_START, WEEK_END)\n",
        "\n",
        "# ===================== 2) Station mapping =====================\n",
        "def _norm(x): return str(x).strip().upper()\n",
        "\n",
        "with open(f\"{SEED}/stations.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    stations_list = json.load(f)\n",
        "\n",
        "sid_by_code, sid_by_name = {}, {}\n",
        "for st in stations_list:\n",
        "    sid  = str(st.get(\"station_id\",\"\")).strip()\n",
        "    code = str(st.get(\"code\",\"\")).strip()\n",
        "    name = str(st.get(\"name\",\"\")).strip()\n",
        "    if code: sid_by_code[_norm(code)] = sid\n",
        "    if name: sid_by_name[_norm(name)] = sid\n",
        "\n",
        "capacity_df = pd.DataFrame(stations_list)[[\"station_id\",\"capacity_station\"]]\n",
        "\n",
        "ALIASES = {\n",
        "    \"AIRPORT T1-2\": \"AIRP_T12\",\n",
        "    \"QASR AL-HOKM\": \"QASR\",\n",
        "    \"NATIONAL MUSEUM\": \"MUSEUM\",\n",
        "    \"WESTERN STATION\": \"S6\",\n",
        "}\n",
        "def resolve_sid(token: str):\n",
        "    t = _norm(token)\n",
        "    if t in sid_by_code: return sid_by_code[t]\n",
        "    if t in sid_by_name: return sid_by_name[t]\n",
        "    if t in ALIASES:\n",
        "        c = _norm(ALIASES[t])\n",
        "        return sid_by_code.get(c, ALIASES[t])\n",
        "    return None\n",
        "\n",
        "# ===================== 3) Events only (holidays disabled) =====================\n",
        "def norm_date(x: str) -> str:\n",
        "    if x is None: return \"\"\n",
        "    s = str(x).strip()\n",
        "    if not s: return \"\"\n",
        "    d = pd.to_datetime(s, errors=\"coerce\", dayfirst=False)\n",
        "    if pd.isna(d):\n",
        "        d = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
        "    return \"\" if pd.isna(d) else d.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "events_csv = f\"{SEED}/calendar_events.csv\"\n",
        "event_rows = []\n",
        "with open(events_csv, \"r\", encoding=\"utf-8\") as f:\n",
        "    rdr = csv.DictReader(f)\n",
        "    cols = {c.lower().strip(): c for c in rdr.fieldnames}\n",
        "    for r in rdr:\n",
        "        event_rows.append({\n",
        "            \"date\": norm_date(r.get(cols.get(\"date\",\"date\"), \"\")),\n",
        "            \"event_type\": (r.get(cols.get(\"event_type\",\"event_type\")) or r.get(cols.get(\"type\",\"type\")) or \"Other\").strip(),\n",
        "            \"stations_impacted\": (r.get(cols.get(\"stations_impacted\",\"stations_impacted\")) or r.get(cols.get(\"stations\",\"stations\")) or \"*\").strip(),\n",
        "            \"demand_modifier\": float((r.get(cols.get(\"demand_modifier\",\"demand_modifier\")) or \"1.0\")),\n",
        "        })\n",
        "\n",
        "GLOBAL_EVENT_TYPES = {\"SaudiNationalDay\"}\n",
        "\n",
        "event_types_map = {}              # (date, SID) -> set(types)\n",
        "event_mult_override = {}          # (date, SID) -> product(mods)\n",
        "global_event_types_by_date = {}   # date -> set(types)\n",
        "global_event_mult_by_date  = {}   # date -> product(mods)\n",
        "\n",
        "for e in event_rows:\n",
        "    d = e[\"date\"]\n",
        "    if not d:\n",
        "        continue\n",
        "    etype = e[\"event_type\"] or \"Other\"\n",
        "    dm    = float(e.get(\"demand_modifier\", 1.0) or 1.0)\n",
        "    tokens = [s.strip() for s in (e[\"stations_impacted\"] or \"*\").split(\";\")]\n",
        "\n",
        "    # Global event?\n",
        "    is_global = (etype in GLOBAL_EVENT_TYPES) or any(_norm(t) in {\"*\", \"ALL\", \"ALL STATIONS\"} for t in tokens)\n",
        "    if is_global:\n",
        "        global_event_types_by_date.setdefault(d, set()).add(etype)\n",
        "        global_event_mult_by_date[d] = global_event_mult_by_date.get(d, 1.0) * dm\n",
        "\n",
        "    # Station-scoped events\n",
        "    for tok in tokens:\n",
        "        if tok == \"\" or _norm(tok) in {\"*\", \"ALL\", \"ALL STATIONS\"}:\n",
        "            continue\n",
        "        sid = resolve_sid(tok)\n",
        "        if sid is None:\n",
        "            print(f\"[warn] Unknown station alias in events CSV: '{tok}'\")\n",
        "            continue\n",
        "        key = (d, _norm(sid))\n",
        "        event_types_map.setdefault(key, set()).add(etype)\n",
        "        event_mult_override[key] = event_mult_override.get(key, 1.0) * dm\n",
        "\n",
        "# Holidays disabled entirely\n",
        "holiday_dates = set()\n",
        "\n",
        "def list_event_types(date_str, sid):\n",
        "    sidn = _norm(sid)\n",
        "    types = set()\n",
        "    if (date_str, sidn) in event_types_map:\n",
        "        types |= event_types_map[(date_str, sidn)]\n",
        "    if date_str in global_event_types_by_date:\n",
        "        types |= global_event_types_by_date[date_str]\n",
        "    return sorted(types)\n",
        "\n",
        "def event_csv_multiplier(date_str, sid):\n",
        "    sidn = _norm(sid)\n",
        "    m = 1.0\n",
        "    if (date_str, sidn) in event_mult_override:\n",
        "        m *= event_mult_override[(date_str, sidn)]\n",
        "    if date_str in global_event_mult_by_date:\n",
        "        m *= global_event_mult_by_date[date_str]\n",
        "    return float(m)\n",
        "\n",
        "# ===================== 4) Modifiers (weekend + events + weather; holidays OFF) =====================\n",
        "mult_cfg     = (config.get(\"multipliers\", {}) or {})\n",
        "weather_mult = mult_cfg.get(\"weather\", {}) or {}\n",
        "events_mult  = mult_cfg.get(\"events\", {}) or {}\n",
        "weekend_mult = float(mult_cfg.get(\"weekend\", 1.0))\n",
        "holiday_mult = 1.0  # force no holiday effect\n",
        "COMBINE_MODE = \"stack\"  # multiply\n",
        "\n",
        "def build_modifier(row):\n",
        "    m = 1.0\n",
        "    # weekend\n",
        "    if int(row.get(\"is_weekend\",0)) == 1:\n",
        "        m *= weekend_mult\n",
        "\n",
        "    # holidays off\n",
        "    hol_m = 1.0\n",
        "\n",
        "    # events: prefer explicit CSV multiplier; fallback to config types\n",
        "    ev_m = event_csv_multiplier(row[\"date\"], row[\"station_id\"])\n",
        "    if ev_m == 1.0:\n",
        "        tmp = 1.0\n",
        "        for t in list_event_types(row[\"date\"], row[\"station_id\"]):\n",
        "            tmp *= float(events_mult.get(t, events_mult.get(\"Other\", 1.0)))\n",
        "        ev_m = tmp if tmp != 1.0 else 1.0\n",
        "\n",
        "    m = m * hol_m * ev_m if COMBINE_MODE == \"stack\" else m * max(hol_m, ev_m)\n",
        "\n",
        "    # weather\n",
        "    w = str(row.get(\"weather_code\", \"\") or \"\")\n",
        "    m *= float(weather_mult.get(w, 1.0))\n",
        "    return float(m)\n",
        "\n",
        "df[\"modifier\"] = df.apply(build_modifier, axis=1)\n",
        "\n",
        "# ===================== 5) Final demand =====================\n",
        "base_demand_safe = pd.to_numeric(df.get(\"base_demand\", 0), errors=\"coerce\").fillna(0)\n",
        "df[\"demand_final\"] = (base_demand_safe * pd.to_numeric(df[\"modifier\"], errors=\"coerce\").fillna(1.0)).fillna(0)\n",
        "\n",
        "# ===================== 6) station_total + crowd_level =====================\n",
        "df = df.merge(capacity_df, on=\"station_id\", how=\"left\")\n",
        "# Normalize by global max instead of per-station max\n",
        "global_max = max(df[\"demand_final\"].max(), 1e-9)\n",
        "df[\"_denom\"] = global_max\n",
        "df[\"demand_norm_final\"] = (df[\"demand_final\"] / df[\"_denom\"]).clip(0, 1)\n",
        "\n",
        "def station_total_from_norm(row):\n",
        "    cap = float(row.get(\"capacity_station\") or 0)\n",
        "    if cap <= 0: return 0\n",
        "    norm = float(row[\"demand_norm_final\"])\n",
        "    evb  = event_csv_multiplier(row[\"date\"], row[\"station_id\"])\n",
        "    # Soft event boost up to +10%\n",
        "    boost = min(1.10, 1.0 if evb <= 1.0 else min(evb, 1.10))\n",
        "    return int(np.round(norm * cap * boost))\n",
        "\n",
        "df[\"station_total\"] = df.apply(station_total_from_norm, axis=1).astype(int)\n",
        "\n",
        "def crowd_from_cap(row):\n",
        "    cap = float(row.get(\"capacity_station\") or 0)\n",
        "    x = float(row.get(\"station_total\") or 0)\n",
        "    if cap <= 0: return \"Medium\"\n",
        "    r = x / cap\n",
        "    if   r < 0.30: return \"Low\"\n",
        "    elif r < 0.60: return \"Medium\"\n",
        "    elif r < 0.85: return \"High\"\n",
        "    else:          return \"Extreme\"\n",
        "\n",
        "df[\"crowd_level\"] = df.apply(crowd_from_cap, axis=1)\n",
        "\n",
        "# ===================== 7) Flags (event/holiday) =====================\n",
        "df[\"special_event_type\"] = df.apply(lambda r: \"+\".join(list_event_types(r[\"date\"], r[\"station_id\"])) or \"None\", axis=1)\n",
        "df[\"event_flag\"]   = (df[\"special_event_type\"] != \"None\").astype(int)\n",
        "df[\"holiday_flag\"] = 0  # holidays disabled\n",
        "\n",
        "# ===================== 8) Headway seconds =====================\n",
        "headway_cfg = config.get(\"headway\", {})\n",
        "peaks_cfg   = config.get(\"peaks\", [])\n",
        "peak_hours  = [int(x.get(\"hour\")) for x in peaks_cfg if \"hour\" in x]\n",
        "peak_hw_min    = float(np.median(headway_cfg.get(\"peak_pattern\",    [7,7,6,8])))\n",
        "offpeak_hw_min = float(np.median(headway_cfg.get(\"offpeak_pattern\", [11,10,12,11])))\n",
        "def hw_for_hour(h): return int(peak_hw_min*60) if int(h) in peak_hours else int(offpeak_hw_min*60)\n",
        "\n",
        "if \"headway_seconds\" in df.columns:\n",
        "    df[\"headway_seconds\"] = pd.to_numeric(df[\"headway_seconds\"], errors=\"coerce\")\n",
        "    mask = df[\"headway_seconds\"].isna()\n",
        "    df.loc[mask, \"headway_seconds\"] = df.loc[mask, \"hour\"].apply(hw_for_hour)\n",
        "else:\n",
        "    df[\"headway_seconds\"] = df[\"hour\"].apply(hw_for_hour)\n",
        "df[\"headway_seconds\"] = df[\"headway_seconds\"].astype(int)\n",
        "\n",
        "# ===================== 9) Export week =====================\n",
        "FINAL_SCHEMA = [\n",
        "    \"date\",\"timestamp\",\"hour\",\"minute_of_day\",\"day_of_week\",\"is_weekend\",\n",
        "    \"station_id\",\n",
        "    \"base_demand\",\"modifier\",\"demand_final\",\n",
        "    \"station_total\",\"crowd_level\",\n",
        "    \"special_event_type\",\"event_flag\",\"holiday_flag\",\n",
        "    \"headway_seconds\"\n",
        "]\n",
        "for c in FINAL_SCHEMA:\n",
        "    if c not in df.columns:\n",
        "        df[c] = np.nan\n",
        "\n",
        "out = df[FINAL_SCHEMA].sort_values([\"date\",\"station_id\",\"minute_of_day\"]).reset_index(drop=True)\n",
        "\n",
        "# QA\n",
        "assert out[\"station_id\"].notna().all()\n",
        "assert (out[\"station_total\"] >= 0).all()\n",
        "\n",
        "# Quick checks\n",
        "print(\"Per-day rows in window:\")\n",
        "print(out[\"date\"].value_counts().sort_index())\n",
        "_23 = out[out[\"date\"]==\"2025-09-23\"]\n",
        "print(\"\\n23-Sep unique stations with event_flag=1:\", _23[_23[\"event_flag\"]==1][\"station_id\"].nunique())\n",
        "\n",
        "# Save\n",
        "OUT_DIR = f\"{ROOT}/data/generated\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "OUT_PATH = f\"{OUT_DIR}/cf_week_2025-09-21_to_27.csv\"\n",
        "out.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved âœ“\", OUT_PATH, \"| Rows:\", len(out), \"| Week:\", WEEK_START.date(), \"â†’\", WEEK_END.date())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d1qttOCCv9q",
        "outputId": "7c18ee74-fc14-4835-e06e-3165d20f8ef2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No expansion needed. All week dates present âœ“\n",
            "Per-day rows in window:\n",
            "date\n",
            "2025-09-21    6486\n",
            "2025-09-22    6486\n",
            "2025-09-23    6486\n",
            "2025-09-24    6486\n",
            "2025-09-25    6486\n",
            "2025-09-26    6486\n",
            "2025-09-27    6486\n",
            "Name: count, dtype: int64\n",
            "\n",
            "23-Sep unique stations with event_flag=1: 6\n",
            "Saved âœ“ /content/2025_GP_28/masar-sim/data/generated/cf_week_2025-09-21_to_27.csv | Rows: 45402 | Week: 2025-09-21 â†’ 2025-09-27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(OUT_PATH)\n",
        "except Exception as e:\n",
        "    print(\"Download skipped (not running in Colab):\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QCpHy5eEEBUj",
        "outputId": "4cdbfff9-5d50-481a-9a87-76b84d8f8c83"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_05a3f7af-0a73-4e68-ab0a-ea1da306aa18\", \"cf_week_2025-09-21_to_27.csv\", 4749932)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}